[{"authors":null,"categories":null,"content":"I am an associate professor of statistics and data science at Winona State University, located in the heart of the Mississippi River driftless region in Winona, MN. My passion is teaching: specifically how to understand data and its limitations; generally how to foster hard thinking and deep understanding. Other professional interests include statistical consulting and data visualization best principles and practice. Check out my Tableau Public profile for examples of my own and student visualizations.\nOutside work, I enjoy spending time outdoors (especially wilderness backpacking); racquetball; baseball; baking sourdough; gardening; board and card games; craft beer; fishing; hunting; and spending time with my wife and two boys.\n","date":1372636800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1372636800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am an associate professor of statistics and data science at Winona State University, located in the heart of the Mississippi River driftless region in Winona, MN. My passion is teaching: specifically how to understand data and its limitations; generally how to foster hard thinking and deep understanding.","tags":null,"title":"Silas Bergen","type":"authors"},{"authors":null,"categories":null,"content":"HOME PAGE\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"cacd8af494d6b28ae3682b1e17f54f22","permalink":"/courses/dsci310/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/dsci310/","section":"courses","summary":"Data visualization","tags":null,"title":"Data visualizationsss","type":"docs"},{"authors":null,"categories":null,"content":"","date":1536451200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536451200,"objectID":"a197073335f65b3e4fbac4115f4fbfb2","permalink":"/courses/dsci310/playday-menu/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/dsci310/playday-menu/","section":"courses","summary":"Data visualization","tags":null,"title":"Data visualizationsss","type":"docs"},{"authors":null,"categories":null,"content":"What happens when you combine the increasing need for carbon-free energy with an increasing eagle population? The potential for some not-so-pretty collisions! For my sabbatical research I have been collaborating with a great group of colleagues from USGS and Conservation Science Global on some fascinating data they\u0026rsquo;ve collected from GPS devices attached to hundreds of bald eagles in the Midwest. (If you prefer listening instead of reading, I have a video presentation essentially narrating this post that I presented at the Wind Wildlife Research Meeting in December 2020.)\n    Eagles flying through a wind farm in Sm√∏la, Noway. Photo by Todd Katzner.  \r\r\rI was super fortunate to have my sabbatical coincide with the 2020-2021 academic year, which was right in the throes of the COVID pandemic. So I got to skip out on all the virtual teaching, whew. Major props to my colleagues here at Winona State and universities everywhere who battled through this year in the trenches; they seriously deserve two sabbaticals after the year they endured. Fortunately the pre-COVID plan already was to collaborate remotely, so the research could proceed in spite of a global pandemic.\n    (Left) An eagle snare is set using a deer carcass (Top right) young eagles tagged with GPS telemetry devices (bottom right) a mature baldy flies away into the snowy day, GPS telemetry device attached. Photos by Mike Lanzone and Trish Miller.  \r\r\rFor my sabbatical I restricted attention to GPS measurements in Iowa, of which there are over 1.7 million for a few dozen eagles over 5 or so years.\n    1.7 million observed bald eagle GPS points across the state of Iowa \r\r\rThe devices record data at 1-11 second intervals while birds are in flight, and from these observations we have the following variables about each point in time:\n Velocity (Kilometers per hour) Angle (in radians); is the bird flying straight or in a \u0026ldquo;tortuous\u0026rdquo; path? Meters above ground level (AGL) Vertical rate (m/s): is the bird increasing or decreasing its AGL?  Our goals with these data are essentially two-pronged:\n Using eagle GPS measurements, can we come up with a way to characterize consistent flight behaviors using the GPS data? Given the flight behaviors we come up with, can we model what underlying land features are related to certain types of behaviors?  For the first question, I used k-means clustering to come up with classifications for every single one of the GPS points into one of k behaviors. So what is k-means clustering? Maybe the animation below will help.\n  \r\rIn this animation we are trying to classify 2-dimensional data (with \u0026lsquo;X\u0026rsquo; and \u0026lsquo;Y\u0026rsquo; coordinates) into one of k=2 clusters. The algorithm iterates between assigning each data point to the cluster with the nearest centroid then recomputing the centroid. This idea extends directly to p-dimensional data.\nChoosing k is a bit of an art, which I won\u0026rsquo;t go into here (we have a forthcoming manuscript you can read for more details), but for our analysis it appeared the k=5 was the appropriate choice. Showing my colleagues boxplots of the GPS variables with cluster indications got them really excited! Here\u0026rsquo;s the fun thing about working with people who know animals well: what, to me, are \u0026ldquo;clusters numbers 1, 2, 3, 4, and 5\u0026rdquo; are, to them, \u0026ldquo;ah! perching!\u0026rdquo; or, \u0026ldquo;gliding from a thermal!\u0026rdquo; So it was they who identified the following actually relevant eagle behaviors from my clusters.\n  \r\rWhat\u0026rsquo;s really cool is watching an eagle \u0026ldquo;fly\u0026rdquo; and seeing these behaviors represented in action. Take a look.\n  \r\rHere you can really see the different behavioral modes as defined by the k-means clustering. The blue \u0026ldquo;gliding\u0026rdquo; points tend to be straight and descending; the pink \u0026ldquo;at altitude\u0026rdquo; points occur when the bird levels out. The green and yellow flights are much more angular (\u0026ldquo;tortuous\u0026rdquo; as my eagle expert colleagues like to say), with the yellow \u0026ldquo;gaining altitude\u0026rdquo; points more obviously doing just that: going \u0026ldquo;up.\u0026rdquo; All of this with a little unsupervised learning!\nYou might notice the wind turbines at the bottom of the animation, which represent 250 meters AGL. This is the turbine rotor-swept zone (RSZ), within which bald eagles are potentially imperiled. What\u0026rsquo;s more, my biologist colleagues suspect that the green and yellow behaviors (\u0026ldquo;flapping\u0026rdquo; and \u0026ldquo;ascending\u0026rdquo;) are riskier to the eagle than the straighter pink (\u0026ldquo;soaring/flapping at altitude\u0026rdquo;) and blue (\u0026ldquo;gliding from thermal\u0026rdquo;) behaviors, as they are potentially more distracted in the former behaviors. These can be defined to come up with three levels of risk:\n \u0026ldquo;Low risk\u0026rdquo;: any GPS point outside the RSZ (\u0026gt;250m AGL) \u0026ldquo;Moderate risk\u0026rdquo;: any GPS point within the RSZ ($\\leq$ 250m) that is also clustered into the \u0026ldquo;pink\u0026rdquo; or \u0026ldquo;blue\u0026rdquo; cluster \u0026ldquo;High risk\u0026rdquo;: any GPS point within the RSZ that is also clustered in the \u0026ldquo;green\u0026rdquo; or \u0026ldquo;yellow\u0026rdquo; cluster  So there we have it! A framework for classifying every eagle GPS point into one of three risk categories, based on the topological flight characteristics of that point. Next, we need to figure out if and how underlying land features can be used to predict which risk category overhead eagle points are most likely to be engaged in. But that\u0026rsquo;s for a future post!\n","date":1614060000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614060000,"objectID":"b5eb1eca504aed416821f34c169f3fdd","permalink":"/post/sabbatical1/","publishdate":"2021-02-23T00:00:00-06:00","relpermalink":"/post/sabbatical1/","section":"post","summary":"What happens when you combine the increasing need for carbon-free energy with an increasing eagle population? The potential for some not-so-pretty collisions! For my sabbatical research I have been collaborating with a great group of colleagues from USGS and Conservation Science Global on some fascinating data they\u0026rsquo;ve collected from GPS devices attached to hundreds of bald eagles in the Midwest.","tags":null,"title":"A sabbatical of eagles (part 1)","type":"post"},{"authors":null,"categories":null,"content":"This week\u0026rsquo;s MakeoverMonday is a good one. Without further ado, the original visualization by Philip Bump, appearing in his Washington Post article entitled Nearly a quarter of Americans have never experienced the U.S. in a time of peace:\n \rThis graph triggered my pedagogical Pavlovian dog. Not just because it\u0026rsquo;s easy to malign the poor pie chart (of which this graph has 115!), but because I had a hunch that a redesign would reveal features of the data that are obscured above.\nBut first, since we do have pie charts, why not briefly discuss their relative merits and demerits. Pie charts are fine if we want to show portions of a whole, and then only if we have relatively few slices. In each pie above, we have only two slices for most years. So a pie chart would be just fine if, for example, I only cared about showing the relative portion of life lived in war and peace for people born in 1905. The problem is that we don\u0026rsquo;t just have a pie for 1905, but for every year 1905-2019. If we wanted to see how the percents change over time, we have to compare slice sizes across pies, and this is very hard to do perceptually. This draws from the work of William Cleveland, who ranked the elementary perceptual tasks we use to compare quantities. We are very bad at making accurate comparisons when the quantities are encoded as angles (I discuss this in more detail here).\nIn the Washington Post graph, the thing that jumps out at me is the column of all-red on the right, and how much less red there is on the left. It\u0026rsquo;s easy to see that people born since 2001 have lived their entire life during a U.S. war; those born in the early 20th century lived a much smaller portion of their lives during war. What is much less obvious is the nature of the upward trend: was it monotone? Up-and-down? This is hard to tell! It doesn\u0026rsquo;t help that in the original article, with my desktop Chrome browser at normal zoom, the graph is so tall that you have to scroll up and down to take it all in.\nHere is my redesign (Tableau Public link):\nI like to think of area charts as pie charts that work over time. The redesign still shows us what the original did: people born in the early 20th century lived a much smaller portion of their lives during war than people born in 2001. We have preserved the \u0026ldquo;part-of-a-whole\u0026rdquo; story that pie charts have to tell. What the redesign reveals is that the upward trend is not monotone. Rather we see a \u0026ldquo;sawtooth\u0026rdquo; pattern, where the saw teeth peak during wars and and dip following wars. Now that we have mapped these percents to a common vertical axis instead of encoded them as angles in a pie chart, we can perceive differences between them much more clearly. And as a bonus, we don\u0026rsquo;t have to do any scrolling!\nA quick caveat: my redesign only includes the same wars listed in the original article, but you could argue the U.S. has been involved in many wars beyond just the ones shown. Take a look for example at this list from Wikipedia.\n","date":1581660000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581660000,"objectID":"cf6817bb1a7018333b41579595efb00f","permalink":"/post/mm6-2020-uswar/","publishdate":"2020-02-14T00:00:00-06:00","relpermalink":"/post/mm6-2020-uswar/","section":"post","summary":"This week\u0026rsquo;s MakeoverMonday is a good one. Without further ado, the original visualization by Philip Bump, appearing in his Washington Post article entitled Nearly a quarter of Americans have never experienced the U.","tags":["MakeoverMonday","Tableau","data visualization"],"title":"A life of war (MakeoverMonday Week 6)","type":"post"},{"authors":null,"categories":null,"content":"It\u0026rsquo;s been a while since I\u0026rsquo;ve posted! For a while I\u0026rsquo;ve been interested in joining the MakeoverMonday community, a group of data visualizers who come together each week to critique and redesign a visualization provided by Eva Murray and Andy Kriebel. I haven\u0026rsquo;t, due to busyness mostly, but this semester I finally took the dive and signed up! Call it a 2020 resolution. I hope that this will become a regular opportunity for me to keep developing my visualization critique and design skills.\nI enjoyed my first challenge, MakeoverMonday 2020 Week 2, a simple visualization that nonetheless provides a lot to think about and was a challenging redesign.\nHere\u0026rsquo;s the original visualization, Figure 1 from the Environmental Health article The USA lags behind other agricultural nations in banning harmful pesticides:\nThe data provided at data.world for the redesign don\u0026rsquo;t actually contain the same information on counts that we see in the original viz, but rather data on total weight of pesticides that are banned/phased out in other countries that were applied used in the U.S. in 2016.\nMy critiques of the original figure:\n The way the \u0026ldquo;whole\u0026rdquo; is divvied up (whether that \u0026ldquo;whole\u0026rdquo; is number of pesticides or total lbs applied) between EU, Brazil, and China is very unclear. The figure tries to get at intersections by including $\\geq 1$, $\\geq 2$ and All 3, but it\u0026rsquo;s impossible to know which pesticides are banned by (for example) both China and Brazil. It takes a while to understand what the \u0026ldquo;whole\u0026rdquo; is. The categories along the horizontal are a mix of both country labels and number of countries. I don\u0026rsquo;t like this mixing of label types. Only counts of pesticides are shown: no information about amount of pesticides applied is available in the visualization shown. For example, China has banned only 11 pesticides, but does it ban the \u0026ldquo;big ones\u0026rdquo; as applied in the U.S.? We can\u0026rsquo;t tell from the figure.  In my redesign, I had a couple goals to address these critiques:\n Make clear what the \u0026ldquo;whole\u0026rdquo; is. Allow the viewer to see where the intersections in banned pesticides exist. E.g., if a pesticide is banned in the EU, is it also banned in China, or in Brazil? All 3? Visualization amount applied rather than counts.  To accomplish this I had to dig a bit deeper than the data provided on Data World, and found what I was looking for with Additional File 5, Tables S131-S133 from the article. I also had to do a bit of cleaning. Here\u0026rsquo;s a link to the data I ultimately ended up connecting to in Tableau\nFinally, my redesign (Tableau Public version):\nSome design comments:\n This accomplishes (I think) a clear visual of \u0026ldquo;the whole\u0026rdquo;: the giant (perhaps too giant) grey box with \u0026ldquo;328 million pounds\u0026rdquo; are the first things you see If desired, you can see which pesticides are banned in the EU, China, and/or Brazil. This information was obscured before. You can now see that although China has banned fewer pesticides, they are pesticides that are more widely used (at least in the U.S.). A critique of my own redesign: are the percents meaningful? \u0026ldquo;98%\u0026rdquo; is a weird percent: the percent of US-applied pesticides that are banned in the EU. If they were banned in the US, would they be replaced by something else?  Some technical comments:\n The grey box is giant, perhaps too much so, but getting Tableau to label the cells of the treemap in a satisfactory way was a pain. Some of the labels (e.g. Fomesan) I added manually. Getting the four treemaps to divide the pesticides up in the same way, while applying different color schemes, was a pain and I used a hack approach (download the Tableau workbook if you are curious).   R code for creating the .csv I used in my redesign, which begins with a stacked version of Tables S131-S133 from Additional file 5\npest \u0026lt;- read.csv('pesticide_complete_stack.csv')\rlibrary(tidyr) library(dplyr)\rlbs \u0026lt;- pest %\u0026gt;% group_by(Pesticide) %\u0026gt;% summarize(USLbs2016 = mean(USLbs2016))\ruse \u0026lt;- function(col) ifelse(is.na(col),0,1)\rout \u0026lt;- pest %\u0026gt;% spread(key = CountryBanned, value = USLbs2016) %\u0026gt;%\rinner_join(lbs, by = 'Pesticide') %\u0026gt;%\rmutate_at(vars(EU:Brazil), use) %\u0026gt;%\rmutate(nbanned = Brazil + China + EU) %\u0026gt;%\rmutate(used_US = ifelse(USLbs2016\u0026gt; 0, 1, 0)) %\u0026gt;% select(Pesticide, USLbs2016, EU,China,Brazil,nbanned,used_US)\rwrite.csv(out, file = 'pesticide_clean.csv',row.names=FALSE)\r ","date":1579240800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579240800,"objectID":"08ede91572c901cc60c75b50a01d1eda","permalink":"/post/makeovermondayweek2-pesticides/","publishdate":"2020-01-17T00:00:00-06:00","relpermalink":"/post/makeovermondayweek2-pesticides/","section":"post","summary":"It\u0026rsquo;s been a while since I\u0026rsquo;ve posted! For a while I\u0026rsquo;ve been interested in joining the MakeoverMonday community, a group of data visualizers who come together each week to critique and redesign a visualization provided by Eva Murray and Andy Kriebel.","tags":["MakeoverMonday","Tableau","data visualization"],"title":"MakeoverMonday: Pesticide usage in US","type":"post"},{"authors":null,"categories":null,"content":"Yesterday, I was preparing material for STAT 405 (biostatistics) I am teaching this spring, and was on the prowl for something that is an improvement upon the base R summary() function (it doesn\u0026rsquo;t even give standard deviations!). The ideal package would also improve upon the base R table() method, for which getting row and/or column percents is a huge pain. Base function xtabs() is great for getting arrays of contigency tables, but no percents. My first stop was the Hmisc package, which has a good summary method via its describe() function.\nTo demonstrate I use the Western Collaborative Group Survey (WCGS) data from Eric Vittingoff\u0026rsquo;s excellent book Regression Methods in Biostatistics.\nHmisc::describe(wcgs[,1:5])\r ## wcgs[, 1:5] ## ## 5 Variables 3154 Observations\r## --------------------------------------------------------------------------------\r## age ## n missing distinct Info Mean Gmd .05 .10 ## 3154 0 21 0.996 46.28 6.256 39 40 ## .25 .50 .75 .90 .95 ## 42 45 50 55 57 ## ## lowest : 39 40 41 42 43, highest: 55 56 57 58 59\r## --------------------------------------------------------------------------------\r## arcus ## n missing distinct Info Sum Mean Gmd ## 3152 2 2 0.628 941 0.2985 0.419 ## ## --------------------------------------------------------------------------------\r## behpat ## n missing distinct ## 3154 0 4 ## ## Value A1 A2 B3 B4\r## Frequency 264 1325 1216 349\r## Proportion 0.084 0.420 0.386 0.111\r## --------------------------------------------------------------------------------\r## bmi ## n missing distinct Info Mean Gmd .05 .10 ## 3154 0 679 1 24.52 2.803 20.59 21.52 ## .25 .50 .75 .90 .95 ## 22.96 24.39 25.84 27.45 28.73 ## ## lowest : 11.19061 15.66050 16.87200 17.21633 17.22242\r## highest: 36.04248 37.22973 37.24805 37.65281 38.94737\r## --------------------------------------------------------------------------------\r## chd69 ## n missing distinct ## 3154 0 2 ## ## Value No Yes\r## Frequency 2897 257\r## Proportion 0.919 0.081\r## --------------------------------------------------------------------------------\r It also has a summary method for objects of class formula which ultimately can be used to create tables that are ready for markdown:\nsummary(chd69~agec, data = wcgs,method='reverse')\r ## ## ## Descriptive Statistics by chd69\r## ## +------------+-----------+-----------+\r## | |No |Yes |\r## | |(N=2897) |(N=257) |\r## +------------+-----------+-----------+\r## |agec : 35-40|18% ( 512)|12% ( 31)|\r## +------------+-----------+-----------+\r## | 41-45 |36% (1036)|21% ( 55)|\r## +------------+-----------+-----------+\r## | 46-50 |23% ( 680)|27% ( 70)|\r## +------------+-----------+-----------+\r## | 51-55 |16% ( 463)|25% ( 65)|\r## +------------+-----------+-----------+\r## | 56-60 | 7% ( 206)|14% ( 36)|\r## +------------+-----------+-----------+\r It does not work as well as you would expect for additional dimensions, however:\nsummary(chd69~agec+behpat, data = wcgs,method='reverse')\r ## ## ## Descriptive Statistics by chd69\r## ## +------------+-----------+-----------+\r## | |No |Yes |\r## | |(N=2897) |(N=257) |\r## +------------+-----------+-----------+\r## |agec : 35-40|18% ( 512)|12% ( 31)|\r## +------------+-----------+-----------+\r## | 41-45 |36% (1036)|21% ( 55)|\r## +------------+-----------+-----------+\r## | 46-50 |23% ( 680)|27% ( 70)|\r## +------------+-----------+-----------+\r## | 51-55 |16% ( 463)|25% ( 65)|\r## +------------+-----------+-----------+\r## | 56-60 | 7% ( 206)|14% ( 36)|\r## +------------+-----------+-----------+\r## |behpat : A1 | 8% ( 234)|12% ( 30)|\r## +------------+-----------+-----------+\r## | A2 |41% (1177)|58% ( 148)|\r## +------------+-----------+-----------+\r## | B3 |40% (1155)|24% ( 61)|\r## +------------+-----------+-----------+\r## | B4 |11% ( 331)| 7% ( 18)|\r## +------------+-----------+-----------+\r Enter summarytools, immediately appealing in its simplicity. Indeed it only has four primary functions, centered on its wonderful dfSummary() function:\nlibrary(summarytools)\r ## Registered S3 method overwritten by 'pryr':\r## method from\r## print.bytes Rcpp\r ## For best results, restart R session and update pander using devtools:: or remotes::install_github('rapporter/pander')\r This is the prettiest, most thorough output I\u0026rsquo;ve come across in a summary function, complete with ASCII bar graphs or histograms representing categorical or quantitative variables. You can prettify it even further in the browser with the view() command:\nview(dfSummary(wcgs))\r Two-way tables come by way of ctable():\nctable(wcgs$agec,wcgs$chd69)\r ## Cross-Tabulation, Row Proportions ## agec * chd69 ## Data Frame: wcgs ## ## ------- ------- -------------- ------------- ---------------\r## chd69 No Yes Total\r## agec ## 35-40 512 (94.3%) 31 ( 5.7%) 543 (100.0%)\r## 41-45 1036 (95.0%) 55 ( 5.0%) 1091 (100.0%)\r## 46-50 680 (90.7%) 70 ( 9.3%) 750 (100.0%)\r## 51-55 463 (87.7%) 65 (12.3%) 528 (100.0%)\r## 56-60 206 (85.1%) 36 (14.9%) 242 (100.0%)\r## Total 2897 (91.9%) 257 ( 8.1%) 3154 (100.0%)\r## ------- ------- -------------- ------------- ---------------\r A couple drawbacks to summarytools: (1) It is not very compatible with the tidyverse, as you can see with the above use of ctable(). (2) Even the nice 2x2 table is not easily extentable to higher dimensions. You could use by(), but\u0026hellip;who wants to do that?\nIn finishing this post I see Adam Medcalf from Dabbling with Data has a nice post on Hmisc, summarytools, and a couple others as well. For my money, I\u0026rsquo;ll take summarytools, though I wish its beautiful 2x2 table displays were more easily extended and its ctable() and descr() functions more tidyverseable!\n","date":1546905600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546905600,"objectID":"b6ed66e6b1f881bdfaa3761340c07b3f","permalink":"/post/summarytools/","publishdate":"2019-01-08T00:00:00Z","relpermalink":"/post/summarytools/","section":"post","summary":"Yesterday, I was preparing material for STAT 405 (biostatistics) I am teaching this spring, and was on the prowl for something that is an improvement upon the base R summary() function (it doesn\u0026rsquo;t even give standard deviations!","tags":["R"],"title":"A word in favor of summarytools","type":"post"},{"authors":null,"categories":null,"content":"This past semester I taught our STAT 370 (statistical consulting and communication) for the first time. This course gave student experience consulting for real clients from the university and community and focused on communicating with a client as well as report and presentation preparation best practices. Most of the required analyses were simple: paired t-tests, simple linear regression, etc. What struck me was the nontrivality of the data tidying process! While STAT 370 is taken mostly by our statistics majors, so many of the examples we encountered would be beautiful case studies for our introductory DSCI (data science) curriculum. In this post I present an actual example from a client that illustrates this.\nThe data here concerned undergraduate nursing students in one of four terms of the nursing program. Of interest was measuring the students' resiliency as measured by the Connor-Davidson Resiliency Scale (CDRS), both prior to and following impelementation of a Stress Management and Resiliency Training (SMART). The client was interested in determining for which of the four terms was there a significant change in resilience.\nHere are the data we\u0026rsquo;re working with (link to pre | link to post). id stands for a unique student identifier. We also have responses to 18 of the CDRS items (each on a 5-point Likert scale). The post data set also contains the student terms; notably this information is not available in the pre data set.\nlibrary(dplyr)\rlibrary(tidyr)\rlibrary(ggplot2)\rpre \u0026lt;- read.csv('cdrs_data_pre.csv')\rpost \u0026lt;- read.csv('cdrs_data_post.csv')\rhead(post)\rnames(pre) #missing term information!!\r ## id Term CDRS_Q1 CDRS_Q2 CDRS_Q3 CDRS_Q4 CDRS_Q5 CDRS_Q6 CDRS_Q7 CDRS_Q8\r## 1 23 Term1 4 4 3 4 4 3 3 4\r## 2 183 Term1 3 4 4 3 4 3 4 2\r## 3 80 Term1 3 4 4 3 4 3 4 3\r## 4 1 Term1 3 4 4 3 3 2 4 4\r## 5 166 Term1 3 4 3 4 4 4 4 4\r## 6 15 Term1 3 3 2 3 3 2 3 3\r## CDRS_Q9 CDRS_Q10 CDRS_Q11 CDRS_Q12 CDRS_Q20 CDRS_Q21 CDRS_Q22 CDRS_Q23\r## 1 4 4 4 3 2 3 3 3\r## 2 3 3 3 3 3 3 3 3\r## 3 4 4 4 4 4 4 4 3\r## 4 4 4 4 4 2 3 3 2\r## 5 4 4 4 4 3 4 4 4\r## 6 2 3 3 3 2 3 3 3\r## CDRS_Q24 CDRS_Q25\r## 1 4 4\r## 2 4 4\r## 3 4 4\r## 4 3 4\r## 5 4 4\r## 6 3 3\r ## [1] \u0026quot;id\u0026quot; \u0026quot;CDRS_Q1\u0026quot; \u0026quot;CDRS_Q2\u0026quot; \u0026quot;CDRS_Q3\u0026quot; \u0026quot;CDRS_Q4\u0026quot; \u0026quot;CDRS_Q5\u0026quot; ## [7] \u0026quot;CDRS_Q6\u0026quot; \u0026quot;CDRS_Q7\u0026quot; \u0026quot;CDRS_Q8\u0026quot; \u0026quot;CDRS_Q9\u0026quot; \u0026quot;CDRS_Q10\u0026quot; \u0026quot;CDRS_Q11\u0026quot;\r## [13] \u0026quot;CDRS_Q12\u0026quot; \u0026quot;CDRS_Q20\u0026quot; \u0026quot;CDRS_Q21\u0026quot; \u0026quot;CDRS_Q22\u0026quot; \u0026quot;CDRS_Q23\u0026quot; \u0026quot;CDRS_Q24\u0026quot;\r## [19] \u0026quot;CDRS_Q25\u0026quot;\r Note that the data are not tidy in the sense that each row is a person, and we have variable information on the questions in columns. I\u0026rsquo;ll return to this in a bit.\nHere ultimately is a visualization that we could use to determine for which terms are the SMART effects strongest, and for which terms is the effect statistically significant:\nWe can see that the average resilience pre-SMART was lower than average resilience post-SMART, and that these differences were most extreme for students in Terms 2 and 3 (which were also the only statistically significant differences). Additionally, students in Term 4 had very high pre- and post-SMART resilience (they\u0026rsquo;re seasoned veterans, after all!)\nA simple plot, with a simple interpretation. But the path to get there is anything but! To create this plot we need:\n to average the 18 CDRS items for each student; join the data sets; compute paired t-tests for each term; prepare data for plotting; plot  So, let\u0026rsquo;s proceed!\nAverage the CDRS items This is perhaps the most interesting step in the process. As mentioned earlier, the data are not tidy in the sense that we have variable information in columns instead of rows. We could reshape (\u0026ldquo;gather\u0026rdquo; or \u0026ldquo;melt\u0026rdquo;) to average CDRS score by term. Doing this for the post data set:\npost_melt \u0026lt;- post %\u0026gt;% gather(key = 'Question',value='Response',CDRS_Q1:CDRS_Q25)\rhead(post_melt)\r ## id Term Question Response\r## 1 23 Term1 CDRS_Q1 4\r## 2 183 Term1 CDRS_Q1 3\r## 3 80 Term1 CDRS_Q1 3\r## 4 1 Term1 CDRS_Q1 3\r## 5 166 Term1 CDRS_Q1 3\r## 6 15 Term1 CDRS_Q1 3\r post_melt %\u0026gt;% group_by(id,Term) %\u0026gt;%\rsummarize(post_mean = mean(Response))\r ## `summarise()` regrouping output by 'id' (override with `.groups` argument)\r ## # A tibble: 67 x 3\r## # Groups: id [67]\r## id Term post_mean\r## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt;\r## 1 1 Term1 3.33\r## 2 2 Term3 3 ## 3 4 Term2 2.83\r## 4 8 Term2 3.11\r## 5 9 Term1 2.72\r## 6 13 Term4 2.72\r## 7 15 Term1 2.78\r## 8 20 Term1 3 ## 9 23 Term1 3.5 ## 10 26 Term4 2.56\r## # ... with 57 more rows\r We could also forego the melting, since ultimately all we want is the average response for each student. We can use the original post data set, and an application of the rowMeans() function within which is nested the select() command:\npost2 \u0026lt;- post %\u0026gt;%\rmutate(post_mean = rowMeans(select(.,CDRS_Q1:CDRS_Q25))) %\u0026gt;%\rselect(id, Term, post_mean)\rhead(post2)\r ## id Term post_mean\r## 1 23 Term1 3.500000\r## 2 183 Term1 3.277778\r## 3 80 Term1 3.722222\r## 4 1 Term1 3.333333\r## 5 166 Term1 3.833333\r## 6 15 Term1 2.777778\r One could argue that the first approach (using gather()) is the best, since it first creates a \u0026ldquo;tidy\u0026rdquo; data set and is arguably more readable. But the second appears more succinct. I\u0026rsquo;ll use the second approach to carry out the same task on the pre data:\npre2 \u0026lt;- pre %\u0026gt;%\rmutate(pre_mean = rowMeans(select(.,CDRS_Q1:CDRS_Q25))) %\u0026gt;%\rselect(id, pre_mean)\r Join pre and post This step is easy!\nboth \u0026lt;- inner_join(pre2,post2,by='id')\rhead(both)\r ## id pre_mean Term post_mean\r## 1 1 3.333333 Term1 3.333333\r## 2 2 2.944444 Term3 3.000000\r## 3 4 2.888889 Term2 2.833333\r## 4 8 2.944444 Term2 3.111111\r## 5 9 2.444444 Term1 2.722222\r## 6 13 2.944444 Term4 2.722222\r Carry out paired t-tests by term Here is the lone \u0026ldquo;formal\u0026rdquo; statistical aspect of the whole problem! We can carry out paired t-tests by term as follows using some ugly base-R code: the by() command. I won\u0026rsquo;t show the output, but here we can see that only for Terms 2 and 3 is the SMART effect statistically significant:\nby(both, both$Term, function(df) t.test(df$pre_mean,df$post_mean,paired=TRUE))\r Prepare data for plotting Referring back to the figure, the geometric mapping includes four points for the four pre-SMART CDRS averages (one for each term); four points for the post-SMART CDRS averages (one for each term); and a line segment connecting them. We can use the joined data set to form our \u0026ldquo;plotting\u0026rdquo; data set.\ntoplot \u0026lt;- both %\u0026gt;%\rgroup_by(Term) %\u0026gt;%\rsummarize(avg_pre = mean(pre_mean),avg_post = mean(post_mean)) %\u0026gt;% mutate(sig = c('no','yes','yes','no'))\r ## `summarise()` ungrouping output (override with `.groups` argument)\r toplot\r ## # A tibble: 4 x 4\r## Term avg_pre avg_post sig ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt;\r## 1 Term1 2.97 3.10 no ## 2 Term2 2.98 3.16 yes ## 3 Term3 2.95 3.26 yes ## 4 Term4 3.21 3.2 no\r Note that the avg_pre and avg_post columns are in fact \u0026ldquo;means of means,\u0026rdquo; and we have manually added a column to indicate whether the difference was statistically significant.\nPlot! And now the fun begins! Here is ggplot code to create the original figure:\nggplot(data = toplot) + geom_point(aes(x = avg_pre, y = 1:4, shape='a',color=sig),size=2) + geom_point(aes(x = avg_post, y = 1:4,shape='b',color=sig),size=2) + geom_segment(aes(x = avg_pre, xend = avg_post, y=1:4,yend=1:4,color=sig)) + scale_y_reverse() + xlab('average CDRS') + ylab('Term') + scale_shape_manual(name='',values=c('a'=19,'b'=17),labels=c('pre SMART','post SMART')) + scale_color_discrete(name='significant?')\r Moral Wow! The only truly \u0026ldquo;statistical\u0026rdquo; aspect of this whole process was a paired t-test. And even that was a bit tricky: figuring out how to carry out these t-tests by term! In reflecting back on the semester, I am struck that our data science students would do well to take our stat consulting course. They would be kept very interested in data \u0026ldquo;wrangling\u0026rdquo; tasks such as this. On the flip side, it\u0026rsquo;s an invaluable experience for our STAT majors (who comprise the usual audience of this course) to realize that the formal statistical analysis in which they are most trained is ultimately the last in a long series of data wrangling steps.\n","date":1546387200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546387200,"objectID":"226616d05add9ba3d6da1fec562c4f36","permalink":"/post/dsci-consulting/","publishdate":"2019-01-02T00:00:00Z","relpermalink":"/post/dsci-consulting/","section":"post","summary":"This past semester I taught our STAT 370 (statistical consulting and communication) for the first time. This course gave student experience consulting for real clients from the university and community and focused on communicating with a client as well as report and presentation preparation best practices.","tags":["education","R","dplyr","ggplot"],"title":"Stat consulting: a data science playground","type":"post"},{"authors":null,"categories":null,"content":"This July, my colleage Todd Iverson and I had the incredible opportunity to lead a pre-conference workshop at ICOTS 10 in Kyoto, Japan. Our workshop was titled Data visualization: best practices and principles using Tableau Public and Python.\nOur workshop began by covering Leland Wilkinson\u0026rsquo;s grammar of graphics. Most data visualization software (Tableau, Python, R, JMP) employ some version of this grammar, and with a firm understanding it becomes easy to transition between them. We focused primarily on Tableau and Python in our workshop. All the workshop materials are available at the designated Github repository.\nIt was an engaging four hours that felt more like one hour, with participants from all over the world! Our fantastic crew:\n\r\r\r ICOTS 10 itself was fantastic, including keynotes from Chris Wild, Hillary Parker, and Anna Rosling R√∂nnlund. And of course, we made time to explore and sample the local cuisine.\n","date":1531544400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1531544400,"objectID":"6fc0f37c1f3216cdc5504c8204bb668d","permalink":"/post/icots/","publishdate":"2018-07-14T00:00:00-05:00","relpermalink":"/post/icots/","section":"post","summary":"This July, my colleage Todd Iverson and I had the incredible opportunity to lead a pre-conference workshop at ICOTS 10 in Kyoto, Japan. Our workshop was titled Data visualization: best practices and principles using Tableau Public and Python.","tags":["data visualization"],"title":"Data vizzing in Japan","type":"post"},{"authors":null,"categories":null,"content":"This post describes an activity I developed for Stat 310: Intermediate Statistics. This course is the second course on statistics at Winona State. I like to think of it as our \u0026ldquo;introduction to modeling\u0026rdquo; course, and this activity does just that: introduces students to the idea of a statistical model, including model assessment and fitting. The activity actually comes in two parts, administered at different times in the semester. In the first part, I am trying to get students to think about how to assess and compare proposed models using residuals. In the second, students need to fit their own models, and compare performance of fitted models.\n Link to Part 1 Link to Part 2 (Question 3)  The Vitruvian Man is a well-known drawing and study by Leonardo DaVinci:\nThis work is sometimes referred to as Canon of Proportions, and is essentially a series of proposed proportions. My activity focuses on two of these proportions:\n the length of the outspread arms is equal to the height of a man the distance from the elbow to the tip of the hand is a quarter of the height of a man  Part 1: comparing proposed models These proportions proposed by DaVinci are essentially two proposed statistical models! We can test these models by collecting some data. This I did by having the students pair up and measure the following three quantities:\nA. Height;\nB. \u0026ldquo;Wingspan\u0026rdquo; (length of the outspread arms);\nC. \u0026ldquo;Elbow-tip\u0026rdquo; (the distance from the elbow to the tip of the hand).\nWith these three measurements, we can assess which of DaVinci\u0026rsquo;s proposed proportions is \u0026ldquo;best!\u0026rdquo; Notice that his first proportion is like fitting the model:\n$$Height = \\beta_0 + \\beta_1 \\times Wingspan + \\epsilon$$\nIn this model, both the intercept and the slope are fixed with \\(\\beta_0 = 0\\) and \\(\\beta_1 = 1\\).\nThe second model is:\n$$Height = \\beta_0 + \\beta_1 \\times ElbowTip + \\epsilon$$\nAgain in this model, the model parameters are fixed with \\(\\beta_0 = 0\\) and \\(\\beta_1 = 4\\).\nSo which model is better?! This motivates finding the modeled \\(\\widehat{Height}\\) given each equation, and comparing the sum of squared residuals, SSError.\nBut first, let\u0026rsquo;s visualize! Here is a scatterplot of Height vs Wingspan using the data collected by the 22 students in my Spring 2018 section of Stat 310. The line indicates the proposed model with \\(\\beta_0 = 0\\) and \\(\\beta_1 = 1\\):\nThe model is about perfect for two students; but clearly imperfect for the other 20. What about Elbow-Tip?\nClearly, this fit looks worse. We can quantify this by computing SSError, which equals 156 using wingspan and 504 using Elbow-Tip.\nPart 2: Fitting simple linear regression models So, DaVinci\u0026rsquo;s proposed model using Wingspan wasn\u0026rsquo;t horrible, but the proposal using Elbow-Tip was. Can we improve these proposed proportions by fitting simple linear regression models, and if so, which fitted model is best?\nThe figure below shows the actual height plotted versus the fitted heights from the two mdoels, along with the (0,1) line:\nIt\u0026rsquo;s difficult to tell which model performs best! Here we really do need the SSErrors, which are 117.8 using wingspan and 122.4 using Elbow-Tip. So, close! But Wingspan slightly out-performs Elbow-Tip as a predictor of height. (Of course, these are in-sample SSErrors; a more accurate comparison would cross-validate which we discuss later in the course.)\n","date":1527033600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527033600,"objectID":"afbd896d86b38884f199d6691d7ff0db","permalink":"/post/vitruvian/","publishdate":"2018-05-23T00:00:00Z","relpermalink":"/post/vitruvian/","section":"post","summary":"This post describes an activity I developed for Stat 310: Intermediate Statistics. This course is the second course on statistics at Winona State. I like to think of it as our \u0026ldquo;introduction to modeling\u0026rdquo; course, and this activity does just that: introduces students to the idea of a statistical model, including model assessment and fitting.","tags":["students","education"],"title":"Modeling the Vitruvian Man","type":"post"},{"authors":null,"categories":null,"content":"Yesterday, several students and I traveled to St. Olaf to illustrate several uses of interesting, publicly-available data that can be used to investigate \u0026ldquo;social justice\u0026rdquo; issues. I\u0026rsquo;ve long been meaning to compile all the data sources and some example projects I\u0026rsquo;ve developed over the years, and this talk provided just the right motivation. Accordingly, I have compiled a list of some of my favorite data sources and example projects and student work. It takes some time to gather, wrangle, and aggregate some of these data sources. Part of my goal is to share some of the work I have already done to clean some of the messier data sets and pare them down to a more ready-to-use format.\nMuch thanks goes to Dr. Julie Legler from St. Olaf for reaching out and inviting us to dialogue with her students, and for encouraging me to actually put in the work to compile all these resources!\n","date":1521090000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1521090000,"objectID":"05db84a6e96d64776d7cf9e0f38e5e13","permalink":"/post/about-social-justice/","publishdate":"2018-03-15T00:00:00-05:00","relpermalink":"/post/about-social-justice/","section":"post","summary":"Yesterday, several students and I traveled to St. Olaf to illustrate several uses of interesting, publicly-available data that can be used to investigate \u0026ldquo;social justice\u0026rdquo; issues. I\u0026rsquo;ve long been meaning to compile all the data sources and some example projects I\u0026rsquo;ve developed over the years, and this talk provided just the right motivation.","tags":["education"],"title":"Public data resources with social justice applications","type":"post"},{"authors":null,"categories":null,"content":"Quick one here. As a statistics educator I am always on the lookout for interesting, real, digestable data that illustrate important statistical concepts. That\u0026rsquo;s a tall order!\nOne site that I visit again and again is this excellent repository hosted at University of Florida. Here\u0026rsquo;s the link. I regularly ping this website for classes ranging from intro stats to experimental design to regression analysis. Not only are they varied in scope and organized by topic, they also have brief descriptions and citations of original sources. It\u0026rsquo;s a gold mine! Hat tip to my colleague Brant Deppa aka Data Hound for originally cluing me in to this website.\nJust an example, here\u0026rsquo;s one on modeling math scores as a function of LSD concentration I recently used in a homework assignment for my intermediate statistics course (spoiler alert: taking LSD is not recommended to improve math test score.)\nlibrary(ggplot2)\rdf \u0026lt;- read.table('http://www.stat.ufl.edu/~winner/data/lsd.dat',header=FALSE,col.names=c('LSD','Score'))\rggplot(data = df,aes(x = LSD, y = Score)) + geom_point() + geom_smooth(method=\u0026quot;lm\u0026quot;) + xlab('LSD concentration (mcg/kg)') + ylab('Math score (out of 100)')\r ## `geom_smooth()` using formula 'y ~ x'\r ","date":1519171200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519171200,"objectID":"9fbf14f696ed30f03dbdd61ebf70d823","permalink":"/post/ufl/","publishdate":"2018-02-21T00:00:00Z","relpermalink":"/post/ufl/","section":"post","summary":"Quick one here. As a statistics educator I am always on the lookout for interesting, real, digestable data that illustrate important statistical concepts. That\u0026rsquo;s a tall order!\nOne site that I visit again and again is this excellent repository hosted at University of Florida.","tags":["education"],"title":"Data set gold mine","type":"post"},{"authors":null,"categories":null,"content":"When temperatures hit 0¬∞F in Minnesota, what better remedy than to head to Florida and talk data science curriculum! The 2-day workshop was held at the New College of Florida in Sarasota, FL. This post reflects some of the ideas circulated at the workshop that stood out to me.\nMultivariate thinking and the introductory statistics and data science course: preparing students to make sense of a world of observational data (Nick Horton) In this talk, Dr. Horton emphasized the fact that most data nowadays is \u0026ldquo;found data\u0026rdquo; of the observational nature. In other words, it is rare to encounter studies that implement careful randomization into groups in order to account for confounding variables. In light of this, he made the following suggestions.\n In introductory statistics classes, focus less on technical assumptions of 2-sample t-test (for example sample size and degrees-of-freedom), and more on issues of confounding and randomization. Bring multivariate thinking into the course early. One easy way this can be done is to introduce data visualization from Day 1. Introductory classes should emphasize writing, projects, and visualization  He also gave several examples of confounding; one can never have too many in their repertoire! An example I really liked: if a study finds that people who use sunscreen tend to have higher rates of skin cancer, would this imply that sunscreen is dangerous to use? The confounding variable in this case would be sun exposure. Of course, people who are using sunscreen are probably experiencing greater sun exposure, which is a risk factor for skin cancer.\nProjects first in an interdisciplinary data science curriculum (Jessen Havill) Dr. Havill gave an overview of the new Data Analytics major at Denison University. The major is intentionally not named Data Science to emphasize the liberal arts nature of the major. It is extremely cross-disciplinary (the two new upper-level Data Analytics courses are taught by an ecologist and an operations researcher). It was interesting to hear about the program at Denison and the thought they put into it. Check out the program website for more details.\nComputer science in the data science curriculum (Panel) This panel included Jessen Havill of Denison University; Dennis F.X. Mathaisel of Babson College; Julie Medero of Harvey Mudd College; and Imad Rahal of St. John\u0026rsquo;s University and The College of St. Benedict. Some pertinent features of the panel:\n  What CS skills are essential for data science?\n The single most important thing, according to Dr. Havill, is abstraction. This concept is more important than the argument of whether this language is better than that language, and is something that can be taught in CS courses from Day 1. Computational thinking that translates a problem into a computational solution, according to Dr. Medero. How to even represent data that comes in nonstandard form, according to Dr. Rahal. The ability to work with data of large Volume, Velocity, and in a wide Varieties of structure.    Are more proprietary tools or more general purpose tools more important?\n The ability to learn something new is more important than expertise in a specific tool, according to Dr. Medero. We will never keep up with all the proprietary tools. The languages I want to use are those that are best for teaching. Choosing a tool because it\u0026rsquo;s hot right now is not necessarily wise, according to Dr. Havill.    Florida Panthers consulting projects (Brian Macdonald) Probably my favorite presentation! Brian is the Director of Hockey Analytics for the Florida Panthers, transitioning toward DIrector of Data Science and Research for the Panthers. In this talk, Brian discussed some fascinating projects he\u0026rsquo;s worked on with students pursuing master\u0026rsquo;s degrees in business analytics.\nIn the first project, he described a model for predicting attendance for games using only information known before tickets go on sale. This will help answer questions like, which games should be in which tiers for variable pricing? What kinds of requests should the team make when the league is developing the schedule? For example, does it make better sense from a sales standpoint to schedule good teams on a Saturday and a bad team during the week, or vice versa?\nThis project used data on announced attendance from nhl.com. Predictors of attendance included day of week, holiday, month, opponent. Interesting nuggets: nobody wants to go to games on Halloween or Easter; and people like to go to games against the \u0026ldquo;original 6\u0026rdquo; NHL teams.\nThe second project centered on understanding what influences season ticket renewal. In short, people who attend more high-scoring, close games that their team wins, are more likely to renew.\nBrian also discussed skills he looks for in interns. He emphasized skills in data management and merging and data visualization more than analysis skills. Coding experience is non-negotiable.\nAnd, of course\u0026hellip; \u0026hellip;there was beach time.\n\r \r\r","date":1515996000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515996000,"objectID":"9ed6bf808ff95feba37b45a80f2ea3d2","permalink":"/post/lads/","publishdate":"2018-01-15T00:00:00-06:00","relpermalink":"/post/lads/","section":"post","summary":"When temperatures hit 0¬∞F in Minnesota, what better remedy than to head to Florida and talk data science curriculum! The 2-day workshop was held at the New College of Florida in Sarasota, FL.","tags":["education"],"title":"Notes from Liberal Arts Data Science Workshop","type":"post"},{"authors":null,"categories":null,"content":"The past two semesters of teaching our lower-level introductory statistics course here at WSU, I\u0026rsquo;ve incorporated in-class group homework. I could go on at length about why *I* think group homework is beneficial, but that\u0026rsquo;s not the point of this post. Rather, this is about what the students think. I think it\u0026rsquo;s quite striking!\nFirst, though, I do need to provide a couple quick details about how I manage group homework. They occur approximately once a week, and I alternate between randomly assigning the students into groups of three and letting them choose their own groups. When students choose their own, I encourage groups of three but allow four (no higher).\nOn end-of-semester course evaluations the past two semesters, I asked the following questions:\n  What do you like BEST about in-class group homework? What did you like LEAST about in-class group homework?   Here is a word cloud of the responses to the first question (what did you like BEST?):\nThere are a lot of words in here that I\u0026rsquo;m glad to see! (And that echo my motivation for implementing group homework.) Students can ask questions; work with people; talk with others; etc. Professor shows up in the cloud too; I\u0026rsquo;m always present and able to help problem-solve, or ask redirecting questions if students are off-topic.\nWell and good. But even more striking is the word cloud for student reponses to what they liked LEAST about group homework:\n## Warning in tm_map.SimpleCorpus(corpus, tm::removePunctuation): transformation\r## drops documents\r ## Warning in tm_map.SimpleCorpus(corpus, function(x) tm::removeWords(x,\r## tm::stopwords())): transformation drops documents\r People didn\u0026rsquo;t work sometimes\u0026hellip;it\u0026rsquo;s even grammatically correct! It\u0026rsquo;s pretty obvious what I need to work on this semester regarding group homework. One possibility to encourage universal participation: I assign the \u0026ldquo;scribe\u0026rdquo; of the group (the person writing down and submitting the group\u0026rsquo;s answers to be graded), and make sure it\u0026rsquo;s someone different each week. Maybe simply paying better attention to students who are \u0026ldquo;staring off\u0026rdquo; and gently encouraging them to participate.\nAlso not surprising, given the groans when I announce it\u0026rsquo;s \u0026ldquo;random assignment week\u0026rdquo;, are the words assigned, random, and randomly. Students aren\u0026rsquo;t fond of being randomly assigned to groups, although it\u0026rsquo;s not something I see myself dropping. It\u0026rsquo;s important to mix up the voices; give international and domestic students a chance to work together; and give students experience working in a team with people they don\u0026rsquo;t necessarily choose (that\u0026rsquo;s real life after all!)\n","date":1515110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515110400,"objectID":"4a9f015d649dd748447e65a88526783b","permalink":"/post/group-homework/","publishdate":"2018-01-05T00:00:00Z","relpermalink":"/post/group-homework/","section":"post","summary":"The past two semesters of teaching our lower-level introductory statistics course here at WSU, I\u0026rsquo;ve incorporated in-class group homework. I could go on at length about why *I* think group homework is beneficial, but that\u0026rsquo;s not the point of this post.","tags":["students","education"],"title":"What students have to say about group homework","type":"post"},{"authors":null,"categories":null,"content":"One of the first concepts I talk about in my data visualization course is the idea of the elementary perceptual task (EPT), an idea explored in depth by visualization pioneers William S. Cleveland and Robert McGill. Essentially, EPTs are visual building blocks for comparing quantities. The EPTs are summarized nicely in Figure 1 from Graphical Perception: Theory, Experiementation, and Application to the Development of Graphical Methods:\n\r\r\r\rFor example, looking at the two dots in the upper-left pane, we perceive that the top dot represents a larger quantity than the bottom dot, because it is higher on a common scale than the bottom dot. In the middle panel (\u0026ldquo;Angle\u0026rdquo;), we perceive that the angle on the right represents a larger quantity than the angle on the left, since it is a larger angle.\nA fundamental finding from the article is that, as humans, we are better at some elementary perceptual tasks than at others. We more accurately and easily compare quantities if they are mapped using position on a common scale than if they are mapped using length, angle or size. In fact, Cleveland and McGill came up with the following ranking of EPTs we perform most efficiently when visually comparing quantities:\n Position along a common scale; Position along misaligned scales; Length; Angles; Size; Color  Recently, I wrote a post discussing a few of the Gestalt principles and illustrating them with some ACS data on income, sex, and field of degree. The Gestalt principles describe how we perceive patterns, while the EPTs describe how we most efficiently compare quantities. In this post, we continue to use income data from the American Community Survey to illustrate EPTs.\nCase study: employment level by sex In my previous post, it was apparent that a gender gap in average annual income persisted even when accounting for year, highest degree, and field of degree. Another important factor that explains income is level of employment, as measured by average hours worked per week. The data for this example are once again aggregated ACS data from the IPUMS USA extract system, filtered to include only employed individuals with at least a Bachelor\u0026rsquo;s degree.\nLet\u0026rsquo;s start by sticking yet another pitchfork in the favorite straw man of data visualization: the pie chart. With an understanding of EPTs, we can begin to understand why pie charts are so maligned in the data visualization community. Take a look at the graph below, and try to determine how the percent of females working full time changes over the years:\nPie charts are fine for visualizing parts of a single whole (it is easy to tell that, in any given year, a majority of women work full time), but they make it difficult to compare parts of different wholes (how the percent working full time changes over the years). This comparison is difficult to make because we are comparing angles, an elementary perceptual task that we do not perform very efficiently or well. Here are the same data, different graph:\nNotice that it\u0026rsquo;s now much easier to discern that the percent of females working 40 or more hours per week is increasing over time, while the percent of females working part-time is decreasing. We now compare the position of the quantities denoted by points along a common scale (the vertical Y-axis), rather than the angle. We can also still clearly see that in each year, a majority of women work full time, by comparing the points along the common vertical scale within a single year.\nLet\u0026rsquo;s turn now to comparing the hours worked for females to males, in 2016 alone. We\u0026rsquo;ll use a bar chart for this: What EPT are we using to compare the percents? It is tempting to think that when we compare quantities in bar charts, we are comparing lengths of the bars. But this is not the primary EPT we are using, here. We are really comparing position along a comon scale once again. This is clearer if we represent the quantities not as bars, but as points. We are carrying out the same exact elementary perceptual task to compare the percents:\nThis \u0026ldquo;point chart\u0026rdquo; has a much better data-to-ink ratio than the bar chart, a concept coined by Edward Tufte. One could argue that it is cleaner and more succinct; better emphasizes the data; and exploits the same exact EPT as the bar chart. Why, then, are bar charts so pervasive while these \u0026ldquo;point charts\u0026rdquo; are not? I think the reason is two-fold. First, we just have a comfort level with bar charts. The point of a data visualization is communication: if we can communicate quicker with a more familiar medium, that has advantages. Second, perhaps more importantly, when we see points we immediately start looking for trends. Direction is one of Cleveland and McGill\u0026rsquo;s EPTs; we are more inclined to try to visually connect points than the tops of bars. However, we shouldn\u0026rsquo;t always connect points, especially if the categories on the horizontal have no sense of ordering (for example, if \u0026ldquo;race,\u0026rdquo; which has no ordering, was on the horizontal instead of employment level).\nFor these reasons, let\u0026rsquo;s go back to the bar chart.\nWhat comparison does this encourage us to make? Because of the two separate panels for females and males (the Gestalt principle of enclosure), the encouraged comparisons seem to be within sex: among females, a much greater percent of them work 40 or more hours a week than are in the other 3 categories; the same can be said for males. It is more likely that we want to compare across sex: assess differences between males and females in work time. How can we better encourage this comparison? One approach is to group by hours worked, rather than sex:\nThis better encourages us to compare females to males, but it still requires us to jump from panel to panel to make that comparison for each employment level. Another major problem with the above graph is that the percents are calculated by conditioning on sex, whereas the facets make it appear the conditioning is on employment level. For example, when we see two bars representing percents in a single panel, we are tempted to think that the total height of the two bars is 100%, which is clearly not the case. The \u0026ldquo;whole\u0026rdquo; out of which the percents are taken is not at all clear.\nHere\u0026rsquo;s one last take: a stacked bar chart.\nThis accomplishes two objectives previous visualizations lacked: A) making the most important comparison obvious (comparing females to males), and B) making obvious \u0026ldquo;the whole\u0026rdquo; out of which the percents are taken. Note that when comparing females to males, we use position along the common (vertical) axis to compare the percents who work 40 or more hours (by determining which dark blue bar extends highest on the vertical); and to compare the percents who work 20 or fewer hours (by determining which white bar extends lowest on the vertical). However, to compare the percent who work 21-30 or 31-39 across sex, we use length, since the middle bars do not share a common baseline. Although we sacrifice making comparisons using position along a common scale with the stacked bar chart for two of the employment levels, we make up for it with fewer facets; more succinct presentation; and more obvious important comparisons.\nData notes Some of the averages in the data set I used for this post (for example, average income for Alabaman males with a doctoral degree in 2009) are based on very few observations, and should be taken with a very large grain of salt. Another brief data note: to aggregate this data set further, it is important to weight by the N column, which indicates how many individuals in the population the average for that row represents.\nR code If interested, here is the R code for creating the bar charts.\nlibrary(ggplot2)\rlibrary(dplyr)\r#Read in the data; assuming current working directory houses the .csv file\rtmp \u0026lt;- read.csv('ACS-income-data-aggregated.csv')\r#Filter to only 2016; aggregate incomes\rdf \u0026lt;- tmp%\u0026gt;%\rfilter(Year == 2016) %\u0026gt;%\rgroup_by(Sex,Hours.work) %\u0026gt;%\rsummarize(count = sum(N), avg.income = weighted.mean(avg.income, N)) %\u0026gt;%\rfilter(Hours.work != \u0026quot;\u0026quot;)%\u0026gt;%\rmutate(pct = count/sum(count))\r#Side by side; faceted by sex:\rggplot(data = df) + geom_bar(aes(x = Hours.work, y = 100*pct, fill=Hours.work), stat='identity') + scale_fill_brewer(palette='Blues') +\rguides(fill=FALSE) +\rtheme_dark()+\rggtitle('Employment level by sex: Bar chart') +\rfacet_wrap(~Sex)+\rtheme(panel.grid=element_blank()) + xlab('Employment level') + ylab('Percent')+\rylim(c(0,100))\r#Side by side; faceted by employment level:\rggplot(data = df) + geom_bar(aes(x = Sex, y = 100*pct, fill=Hours.work), stat='identity') + scale_fill_brewer(palette='Blues') +\rguides(fill=FALSE) +\rtheme_dark()+\rfacet_grid(.~Hours.work)+\rtheme(panel.grid=element_blank()) + xlab('Employment level') + ylab('Percent')+\rylim(c(0,100))\r#Stacked:\rggplot(data = df) + geom_bar(aes(x = Sex, y = 100*pct, fill=Hours.work), stat='identity') + scale_fill_brewer(name='Employment level',palette='Blues') +\rtheme_dark()+\rtheme(panel.grid=element_blank()) + xlab('Employment level') + ylab('Percent')+\rylim(c(0,100))\r ","date":1513641600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1513641600,"objectID":"879296743387af900316a7e9724a2c85","permalink":"/post/on-epts/","publishdate":"2017-12-19T00:00:00Z","relpermalink":"/post/on-epts/","section":"post","summary":"One of the first concepts I talk about in my data visualization course is the idea of the elementary perceptual task (EPT), an idea explored in depth by visualization pioneers William S.","tags":["data visualization"],"title":"On elementary perceptual tasks","type":"post"},{"authors":null,"categories":null,"content":"The fall semester is over and final grades are in, which means it\u0026rsquo;s time to reflect on what just took place and how to grow from here. Today, I reflect on my third time teaching the data visualization course. This course has come a long way since the first time I taught it in Fall 2015, and yet there are still so many improvements to make! One of the concepts I want to greater emphasize next time I teach the course are the Gestalt principles, which Gestalt psychologist Kurt Koffka summarizes as the idea that \u0026ldquo;The whole is other than the sum of the parts.\u0026quot;. I like to think of the Gestalt principles as ground rules for how to create meaningful patterns out of chaos.\nTwain Taylor has an excellent post on how the Gestalt principles manifest in data visualization. He summarizes the principles in this chart:\nQuoting from his post;\n Here is what we notice from each of the illustrations:\n   Proximity: We see three rows of dots instead of four columns of dots because they are closer horizontally than vertically. Similarity: We see similar looking objects as part of the same group. Enclosure: We group the first four and and last four dots as two rows instead of eight dots. Symmetry: We see three pairs of symmetrical brackets rather than six individual brackets. Closure: We automatically close the square and circle instead of seeing three disconnected paths. Continuity: We see one continuous path instead of three arbitrary ones. Connection: We group the connected dots as belonging to the same group. Figure \u0026amp; ground: We either notice the two faces, or the vase. Whichever we notice becomes the figure, and the other the ground   In my experience visualizing data, it seems the most pervasive principles are proximity, similarity, enclosure, and connection.\nI was struck by the ubiquitousness of these principles as I was reviewing my students' final visualization projects. One very fine project in particular inspired me to write this post. Next time I teach the visualization course, I hope to include the following example to illustrate these principles to my students.\nIllustrating the Gestalt principles with ACS income data As an example data set, I settled on data over 2009-2016 from the American Community Survey, which I requested from the IPUMS USA data request system. I specifically requested incomes by year, sex, educational attainment, and field of degree. I filtered the data to only include those with a Bachelor\u0026rsquo;s degree or higher who were currently employed at the time of sampling, and created a new variable to indicate whether or not the field was a STEM field (using a combination of this source and this source to help me determine).\nThe primary question I want to visualize is: What is the inequality in average income comparing males to females? Of course, average income depends on many other factors, including field of degree, type of degree, and year, to name just a few. We\u0026rsquo;ll focus on visualizing the gender income gap adjusting for these other factors as well.\nHere\u0026rsquo;s a quick look at the first six rows of the data set.\n## Year Sex Educ Field avg.income\r## 1 2009 Female Bachelor's degree Non-STEM 47440.41\r## 2 2009 Female Bachelor's degree STEM 51423.20\r## 3 2009 Female Doctoral degree Non-STEM 77378.22\r## 4 2009 Female Doctoral degree STEM 87257.88\r## 5 2009 Female Master's degree Non-STEM 60549.57\r## 6 2009 Female Master's degree STEM 65621.16\r Notably, the data set consists of one row per year/sex/education/field category, with avg.income indicating the average income for that combination.\nSo let\u0026rsquo;s visualize! We want to investigate the gender income gap, so here\u0026rsquo;s a first go:\nMostly, this is chaos! But we do see the Gestalt principle of proximity manifest itself: we perceive the incomes on the left (belonging to females) as a group, and the incomes on the right (belonging to males) as a group. Let\u0026rsquo;s incorporate Year on the horizontal, since we are accustomed to identifying trends across time:\nWe now see year groupings by way of the proximity principle, and sex groupings via color-coding with the similarity principle. There\u0026rsquo;s still too much chaos, however. Let\u0026rsquo;s try again:\nThe amount of chaos is reduced dramatically, all by way of implementing the enclosure principle, specifically enclosing the highest level of educational attainments together in separate panels. This particular type of enclosure is often referred to as faceting in the data visualization realm. The drastic reduction in chaos, and improvement of clarity, is due to the fact that there were four educational attainment groupings. We would not have improved the clarity as much if we had, say, grouped by field of degree instead, which only has two groups:\nFaceting by educational attainment is better, so let\u0026rsquo;s continue working with that one, now indicating field of degree:\nThe new principle is connection. Clearly, we perceive each line as an entity, representing now a Sex/Field combination (Male/STEM, for example). Another instance of similarity is in play, since the lines for STEM fields are dashed, while the lines for non-STEM fields are solid.\nSo, here we have it, a visualization that illustrates the principles of proximity, similarity, enclosure, and connection. We\u0026rsquo;ve implemented these principles to significantly reduce chaos and improve clarity. But there\u0026rsquo;s still an issue with this visualization. Remember the intent of the visualization is to explore gender income inequality. If we take another look at the above visualization, this is not the most obvious comparison to make. Rather, due to the proximity and similarity of the lines within each sex (they are close together, and of the same color), the comparison our brain is encouraged to make is to compare incomes of STEM to non-STEM, within sex. That\u0026rsquo;s not the most important comparison! It makes most sense to compare males to females, within field.\nThis brings us to a related concept: not all means of introducing similarity are created equal. When we group by similarity, we tend to first recognize similarities in color, then in shape. Angela Wright, a color psychologist, states that \u0026ldquo;color is noticed by the brain before shapes or wording.\u0026quot; Thus if we want the encourage the viewer to compare the sexes, we should probably color-code by field instead, so we compare income by gender within field. Here\u0026rsquo;s how that looks:\nI\u0026rsquo;m still not convinced that this encourages the brain to first compare the sexes to each other. The proximity of the STEM and non-STEM lines is too hard to overcome! We might need to introduce another layer of enclosure (by way of faceting) to make the important comparison the most obvious:\nDetails on data collection and visualization IPUMS stands for Integrated Public-Use Microdata Series. The suite of IPUMS tools is an excellent source of varied data, from health to education to international census data. I aggregated the data for this example from the microdata extract, and you can download a .csv of the aggregated data here.\nR code for creating the \u0026ldquo;final two\u0026rdquo; visualizations is below:\nlibrary(ggplot2)\r#Assuming file is in current working directory:\rstemdata \u0026lt;- read.csv(\u0026quot;ACS-stem-aggregated.csv\u0026quot;)\rstemdata$Educ \u0026lt;- factor(stemdata$Educ, levels = c(\u0026quot;Bachelor's degree\u0026quot;,\u0026quot;Master's degree\u0026quot;,\u0026quot;Doctoral degree\u0026quot;,\u0026quot;Professional degree\u0026quot;))\r#Faceting by education;\r#color-coding by field;\r#different lines for Sex:\rggplot(data = stemdata) + geom_point(aes(x = Year, y = avg.income/1000,color=Field)) +\rgeom_line(aes(x = Year, y = avg.income/1000,color=Field,linetype=Sex)) +\rfacet_grid(.~Educ)+\rylab('Average income (in thousand $)')\r#Double faceting:\rggplot(data = stemdata) + #geom_point(aes(x = Year, y = avg.income/1000, shape=Sex)) +\rgeom_line(aes(x = Year, y = avg.income/1000,linetype=Sex)) +\rfacet_grid(Field~ Educ)+\rylab('Average income (in thousand $)')\r ","date":1513036800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1513036800,"objectID":"4fb5d20a2ce1508e3622c713e0c0cf43","permalink":"/post/income-gestalt/","publishdate":"2017-12-12T00:00:00Z","relpermalink":"/post/income-gestalt/","section":"post","summary":"The fall semester is over and final grades are in, which means it\u0026rsquo;s time to reflect on what just took place and how to grow from here. Today, I reflect on my third time teaching the data visualization course.","tags":["data visualization"],"title":"Gestalt principles and income inequality","type":"post"},{"authors":null,"categories":null,"content":"The midterm project for my data visualization course this past fall required students to submit to the ASA\u0026rsquo;s Police Data Challenge. The competition involved analyzing millions of 911 calls for one of three cities (Baltimore, Cincinnati, or Seattle). I had the students investigate the Seattle data set, since it contained latitudes and longitudes of each call.\nSeveral weeks later, we received the exciting news that one of the teams won \u0026ldquo;Best Overall\u0026rdquo; among undergraduate teams! Congratulations to Winona State students Jimmy Hickey, Kapil Khanal, and Luke Peacock for their excellent work.\n","date":1512626400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512626400,"objectID":"ff7c06f8c665fa7b352d6e7494680ca4","permalink":"/post/policedatachallenge/","publishdate":"2017-12-07T00:00:00-06:00","relpermalink":"/post/policedatachallenge/","section":"post","summary":"The midterm project for my data visualization course this past fall required students to submit to the ASA\u0026rsquo;s Police Data Challenge. The competition involved analyzing millions of 911 calls for one of three cities (Baltimore, Cincinnati, or Seattle).","tags":["students","data visualization"],"title":"And the winner is...","type":"post"},{"authors":null,"categories":null,"content":"   Winona State University undergraduates made a great impression at the 2017 MinneMUDAC data analytics competition   \r\rOn November 3-4, Winona State statistics and data science students participated in the fantastic MinneMUDAC 2017 data analytics competition. Students worked in teams of up to five students for one month analyzing de-identified administrative medical and pharmacy claims data provided by Optum. The competition required students to analyze complicated data on health insurance claims made by Type-II diabetics. The event was hosted by MinneAnalytics at the Optum campus in Bloomington, Minnesota.\nThe event began the evening of November 3. MinneAnalytics provided dinner and a professional Q\u0026amp;A panel of data scientists. The event kicked off in earnest on Saturday, November 4. Students gave their 5-minute presentations to teams of judges, and were scored on a variety of criteria ranging from analytic acumen; presentation organization; and team synergy. Some weight was also given by how accurately teams could predict the top-6000 most expensive Type-II diabetics for the next calendar year for a held-out target data set of patients for which the actual insurance costs were known only to the event organizers.\nThe top five teams from the first round went on to present to all judges in a final round. From this final round, teams were given awards for \u0026ldquo;Best overall\u0026rdquo;; \u0026ldquo;Analytic acumen\u0026rdquo;; and \u0026ldquo;Serendipitous discovery.\u0026rdquo;\nThere were 22 total undergraduate teams who participated. We were very excited that of the top five teams to proceed to the final round, three were from Winona State! Of these three, one won \u0026ldquo;Best overall\u0026rdquo; and another the award for \u0026ldquo;Analytic acumen.\u0026rdquo; We were very proud of all our students who participated. Each one of them took time out of their busy semesters to gain fantastic professional development experience. From real-life data analysis of a very messy and complicated data set to presenting their results to panels of industry and academic professionals, it will be an experience that will serve them all very well.\n    The \"Best Overall\" team at MinneMUDAC 2017, from Winona State University.\rFrom left to right: Sam Meyer; Sam Dokkebakken; Eddie Schmitt; Austin Ellingworth; and Jack Barta \r\r\r   The \"Analytic Acumen\" award went to this team from Winona State University.\rFrom left to right: Reagan Buske; Chris Humbert; Mariah Quam; David Stampley Jr; and McHale Dye.  \r\r","date":1510293600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1510293600,"objectID":"c4ab8bf206379ece79b7c773d7c444d4","permalink":"/post/minnemudac/","publishdate":"2017-11-10T00:00:00-06:00","relpermalink":"/post/minnemudac/","section":"post","summary":"Winona State University undergraduates made a great impression at the 2017 MinneMUDAC data analytics competition   \r\rOn November 3-4, Winona State statistics and data science students participated in the fantastic MinneMUDAC 2017 data analytics competition.","tags":["students"],"title":"Minne MUDAC 2017","type":"post"},{"authors":null,"categories":null,"content":"This play day is modified from a similar assignment by Jerzy Wieczorek of CivilStat.com specifically this one.\nConsider data tablulated from the 2010 Digest of Education Statistics, tables 308 to 330.. The intent of this data is to answer:\n In what fields are more women entering college? How is each field\u0026rsquo;s gender balance changing? Is there a difference in gender balance comparing STEM to non-STEM fields? How has the gender balance in STEM fields changed relative to the balance in non-STEM fields over time?  Here is one take.\nStep 1 Critique this visualization at your tables, making a list of your critiques. Be specific, considering:\n What comparisons do you want to make, and how easy is it to make those comparisons? What are redesigns that would make those comparisons easier by better exploiting principles of visual perception? What aesthetic attributes need changing?  Each student should submit critiques to the Play Day 4 submission folder on D2L.\nStep 2 Create a visualization using the data that improves upon the one provided, incorporating the critiques you made in Step 1. Submit the redesign to the Play Day 4 submission folder on D2L.\n","date":1510012800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1510012800,"objectID":"2f61101b248101a2e2ec97caf0f673d2","permalink":"/courses/dsci310/playday4/","publishdate":"2017-11-07T00:00:00Z","relpermalink":"/courses/dsci310/playday4/","section":"courses","summary":"This play day is modified from a similar assignment by Jerzy Wieczorek of CivilStat.com specifically this one.\nConsider data tablulated from the 2010 Digest of Education Statistics, tables 308 to 330.","tags":null,"title":"Play Day Task","type":"courses"},{"authors":null,"categories":null,"content":"Monday morning, October 30, found me groggy and sandy-eyed. The culprit was the 5-hour and 17-minute, 10-inning thriller between the LA Dodgers and Houston Astros in Game 5 of the 2017 the night before. Thanks to living in the Central Time Zone, I went to bed around 1am. The Astros ended up defeating the Dodgers 13-12, but the game was insane, featuring three comebacks from deficits of 3 runs or more. By the end, I was emotionally exhausted, and I didn\u0026rsquo;t even have a stake in either team! Lots was written about the game over at Fangraphs, one of my favorite baseball analytics websites.\nOne post in particular by Craig Edwards caught my attention from a data visualization perspective. It compared the game to another epic game: Game 6 of the 2011 World Series, where the St. Louis Cardinals staved off elimination by defeating the Texas Rangers 10-9 in 11 innings. The crux of the article was a table that listed the top-20 \u0026ldquo;most exciting events\u0026rdquo; of each World Series side-by-side. Here is a screenshot of said table:\n\r \r\rAn \u0026ldquo;exciting event\u0026rdquo; is one that yields a large swing in the win expectancy of the game, as measured by Win Probability Added, or WPA. Read Fangraph\u0026rsquo;s excellent glossary entry on WPA for more detail, but essentially, the bigger the WPA of an event, the more exciting it is.\nThe table is great, but it\u0026rsquo;s hard to see which of the two games has the most exciting Top-20 events. I wanted to visualize! But I needed the data. Fangraphs has play logs for every single Major League game, which lists (among other metrics) the events of the game; the win expectancy following the event; and the WPA of the event. I needed data from the Game 6, 2011 and the Game 5, 2017 play logs. But I needed them both in the same data source!\nR has a great package rvest that makes web-scraping (especially scraping html tables) quite easy. Here\u0026rsquo;s the code I wrote to scrape the Game 6, 2011 data; do some cleaning; and write the cleaned data into a .csv file. I wrote very similar code to get the Game 5, 2017 data:\nlibrary(rvest)\rlibrary(dplyr)\r#Read in the data, find the right table:\rurl \u0026lt;- 'https://www.fangraphs.com/plays.aspx?date=2011-10-27\u0026amp;team=Cardinals\u0026amp;dh=0\u0026amp;season=2011'\rraw \u0026lt;- read_html(url)%\u0026gt;%\rhtml_table(fill=TRUE)\rmytable \u0026lt;- raw[[9]][,1:12]\r#Use dplyr to clean it up. By code row:\r#Create absolute value of WPA\r#Create new column to indicate the game\r#Remove the \u0026quot;%\u0026quot; from the win expectancy, create Event number\r#Arrange in descending order by WPA\r#Create rank column\rcleantable \u0026lt;- mytable %\u0026gt;%\rmutate(WPA_abs = abs(WPA)) %\u0026gt;% mutate(Game = rep('Game 6, 2011',nrow(mytable))) %\u0026gt;% mutate(WE = as.numeric(gsub('%','',WE)), Event = 1:nrow(mytable)) %\u0026gt;% arrange(-WPA_abs) %\u0026gt;% mutate(WPA_Rank = 1:nrow(mytable)) #Write cleaned data to csv file\rwrite.csv(cleantable,file='WPA_Game6_2011_all.csv',row.names=FALSE)\r Then on to visualizing! Using Tableau, I created a win-expectancy graph for each of the games. The red dots indicate \u0026ldquo;exciting events\u0026rdquo;; events with WPA of 15% or more:\n\r\r\rClearly, both games were crazy, with wild swings in win expectancy! If you count up the red dots, Game 6 in 2011 had 8 \u0026ldquo;exciting events\u0026rdquo; while Game 5 in 2017 had 10 \u0026ldquo;exciting events.\u0026rdquo; 15% is quite an arbitrary threshold for \u0026ldquo;exciting\u0026rdquo; however; different thresholds would likely change the comparison. Comparing total WPA flips the story: the total WPA of 7.2 in Game 6, 2011 was larger than the total WPA of 6.2 in Game 5, 2017.\nMy primary interest, however, was to compare the top-20 most exciting events of both games in a clearer visual manner, respecting principles of human perception:\n\rFrom this graph, we can see the four points on the right that lie below the reference line. These indicate that the top four most exciting events in Game 6, 2011 were more exciting than the top four most exciting events from Game 5, 2017. On the other hand, the top 5-10 most exciting events in Game 5, 2017 lie above the reference line, indicating they were more exciting than the events with WPA ranked 5-10 in Game 6, 2011. The rest of the events ranked 11-20 in WPA go back to lying below the line.\nSo, it does appear that Game 6, 2011 was truly a more thrilling game than Game 5, 2017! The total WPA of Game 6, 2011 was greater than the total WPA in Game 5, 2017, and of the top-20 \u0026ldquo;most exciting\u0026rdquo; events, they tended to be more exciting in 2011. Oh well, Game 5, 2017 was still worth losing a few hours of sleep! I think\u0026hellip;.\n","date":1509426000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1509426000,"objectID":"49ab0553a1318515375d4dee7a9bdfae","permalink":"/post/viz-worldseries-wpa/","publishdate":"2017-10-31T00:00:00-05:00","relpermalink":"/post/viz-worldseries-wpa/","section":"post","summary":"Monday morning, October 30, found me groggy and sandy-eyed. The culprit was the 5-hour and 17-minute, 10-inning thriller between the LA Dodgers and Houston Astros in Game 5 of the 2017 the night before.","tags":["data visualization","R","baseball","web-scraping","Tableau"],"title":"Quantifying thrill","type":"post"},{"authors":null,"categories":null,"content":"Consider the following data on World Migration from the United Nations, Department of Economic and Social Affairs, Population Division, Trends in International Migrant Stock.\nQuestions to be answered visually (no restrictions!):\n Which countries and regions have the most people leaving? Where do they go? Which countries and regions have the most people entering? Where do they come from?  ","date":1508112000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1508112000,"objectID":"9b1eaa781a3d9898bf891fad93cbe289","permalink":"/courses/dsci310/playday3/","publishdate":"2017-10-16T00:00:00Z","relpermalink":"/courses/dsci310/playday3/","section":"courses","summary":"Consider the following data on World Migration from the United Nations, Department of Economic and Social Affairs, Population Division, Trends in International Migrant Stock.\nQuestions to be answered visually (no restrictions!","tags":null,"title":"Play Day Task","type":"courses"},{"authors":null,"categories":null,"content":"For this semester\u0026rsquo;s midterm project, we will be participating in the American Statistical Association\u0026rsquo;s Police Data Challenge. Read the information on the website for many more details. For the midterm project, all students should visualize the Seattle data set.\n  Detailed version of the rules.\n  Fill out declaration of intent form by Friday, October 6. Only one team member needs to submit the form. List me (Silas Bergen) as your sponsor.\n  From the website:\n Awards will be given in three categories (1) Best Overall Analysis, (2) Best Visualization, and (3) Best Use of External Data.\n  Winning teams will receive a $50 Amazon gift card, complimentary memberships to ASA, and a Police Data Challenge 2017 t-shirt, along with bragging rights and a chance to have an impact on local communities.\n  Winners also will be profiled and promoted to ASA\u0026rsquo;s membership of leading statisticians and data scientists in academia, industry, business, and government, and through ASA\u0026rsquo;s public education campaign, ThisisStatistics. Second and third place winners will also be recognized.\n ###Requirements for the competition\nPrepare the materials that you need to submit to the competition: PowerPoint presentation with up to 10 slides, and document with $\\leq 500$ words detailing your process. Each team should submit these files by Nov 3.\n###Additional requirements for DSCI 310 Midterm\nThe main additional requirement is to use Census data to some extent in your project. One way is to merge Census Tract information with the Tract information in the Seattle data file. A very nice R package for downloading Census data is tidycensus, especially the function get_acs(). This function makes use of the Census API, specifically reading in data from the American Community Survey (ACS).\n More information on the ACS API Complete list of 2011-2015 ACS variables Nice overview of tidycensus by its author, Kyle Walker  Additionally, each team will present their PowerPoint slides beginning on October 27 (so really, the deadline to have your PowerPoint done is 10/27, not 11/3).\nRubrics for DSCI 310 midterm  Rubric for the visualizations (50%) Rubric for presentations (30%) Link to group member evaluation (20%)  Relevant resources  Understanding Census Geographic Identifiers  Group assignments Professionally, you will not get to always choose your own groups. For the midterm project you will be assigned into groups, which are listed below. Divide the work, and conquer!\nGroup presentation order:\n4, 2, 7, 1, 9, 8, 6, 5, 3\n  Group 1:\n Jack Barta Sam Meyer Mariah Quam    Group 2:\n Fengrui Xue Thomas Gathje Max Oelerking    Group 3:\n Kapil Khanal Luke Peacock Jimmy Hickey    Group 4:\n Will Diedrick Yulun Xu Adam Clemens Lauren Willkom    Group 5:\n Salman Quraishi Waheed Khan Linh Nguyen    Group 6:\n Nathan Smith Paul Boelter David Stampley Akif Khan    Group 7:\n Austin Ellingworth Brad Erickson Sam Dokkebakken    Group 8:\n Matthew Ladin McHale Dye Catherine Nead    Group 9:\n Stacey Miertschin Reagan Buske Ross Krueger    ","date":1507507200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1507507200,"objectID":"41cd449ac01a9f9e327c5a5abba2e586","permalink":"/courses/dsci310/midterm/","publishdate":"2017-10-09T00:00:00Z","relpermalink":"/courses/dsci310/midterm/","section":"courses","summary":"For this semester\u0026rsquo;s midterm project, we will be participating in the American Statistical Association\u0026rsquo;s Police Data Challenge. Read the information on the website for many more details. For the midterm project, all students should visualize the Seattle data set.","tags":null,"title":"Midterm project","type":"courses"},{"authors":null,"categories":null,"content":"Consider the Major League Baseball Standings data. There are two worksheets in this Excel file: one with 2016 standings, and one with 2017 standings. The important variables in each are the team names (Tm); actual winning percentage (W-L%); and predicted winning percentage (pythWPct), which is each team\u0026rsquo;s predicted winning percentage based on the number of points (\u0026ldquo;runs\u0026rdquo;) scored by and against that team.\nQuestions to be answered (no restrictions!):\n Which teams' winning percentages most improved the most from 2016-2017? Which teams got much worse? Which teams are most underperforming and overperforming their predicted winning percentage in 2017?  ","date":1505260800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1505260800,"objectID":"dd7c79a88cacc898069f82a0c55c2e1c","permalink":"/courses/dsci310/playday1/","publishdate":"2017-09-13T00:00:00Z","relpermalink":"/courses/dsci310/playday1/","section":"courses","summary":"Consider the Major League Baseball Standings data. There are two worksheets in this Excel file: one with 2016 standings, and one with 2017 standings. The important variables in each are the team names (Tm); actual winning percentage (W-L%); and predicted winning percentage (pythWPct), which is each team\u0026rsquo;s predicted winning percentage based on the number of points (\u0026ldquo;runs\u0026rdquo;) scored by and against that team.","tags":null,"title":"Play Day Task 1","type":"courses"},{"authors":null,"categories":null,"content":"This past summer, I along with my colleagues Chris Malone and Brant Deppa had the fantastic opportunity to host four students for a 10-week summer research experience for undergraduates (REU). Winona State was awarded the REU by the American Statistical Association, which had received a grant from the NSF to fund four students at each of nine sites over the course of three years (three different sites per year).\nMegan Aadland (South Dakota State), Jenn Halbleib (Amherst), Adrianna Kallis (Iowa State), and Eva Tourangeau (Lawrence) were selected from a competitive, national pool of undergraduates. Their primary task: to learn! Their specific task was to partner with researchers at the Integrated Public Use Microdata Series International (IPUMS-I) to develop data products and visualizations for IPUMS-I users. Two students worked primarily on using multiple correspondence analysis to develop an index of household wealth, while the other two worked on an interactive data visualization to summarize basic demographic information for countries of interest to IPUMS-I users. Both projects required a lot of data cleaning after extracting the microdata from the IPUMS-I query service!\nAlong with their research, the students took several professional development trips to learn about applications of data analytics in the workplace. These included visits to Mayo Clinic, Be the Match, Optum, and the Wilder Foundation.\nBut it wasn\u0026rsquo;t all work! Fun included:\n  St. Paul Saints baseball game! \r\r  Suncrest pizza farm! \r\rAt the end of the summer, the students traveled to Baltimore to present their research at the 2017 Joint Statistical Meetings.\nThe 10 weeks flew by! The students were a joy to spend a summer with, and it is obvious that great things lie ahead for each of them. The three REU sites for Summer 2018 are already selected, so go apply (or tell your students to apply)!\n","date":1505019600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1505019600,"objectID":"cb80a4f6fffba34016dcb2f2ac4930b1","permalink":"/post/winstats/","publishdate":"2017-09-10T00:00:00-05:00","relpermalink":"/post/winstats/","section":"post","summary":"This past summer, I along with my colleagues Chris Malone and Brant Deppa had the fantastic opportunity to host four students for a 10-week summer research experience for undergraduates (REU). Winona State was awarded the REU by the American Statistical Association, which had received a grant from the NSF to fund four students at each of nine sites over the course of three years (three different sites per year).","tags":["students"],"title":"A summer REU at Winona State","type":"post"},{"authors":null,"categories":null,"content":"Data summarization and visualization (Fall 2017) Course overview The amount and extent of data in our world is growing extremely rapidly. As such there is an increasing need for analysts skilled in summarizing data. Visualizations are an immensely popular way of summarizing data, as they allow viewers to efficiently translate data into information. Modern visualizations are either static or interactive, with a growing trend towards the latter. In this course we will discuss how we perceive information visually, how to identify and avoid data visualization pitfalls, and how to create effective static and interactive visualizations.\nPrerequisites Data Science 210. In DSCI 310 I will assume you have working knowledge of data management tools and techniques. Some tasks in DSCI 310 will require a nontrivial amount of data manipulation and cleaning. While I will provide support for this as needed, I will expect you to be able to obtain and \u0026ldquo;wrangle\u0026rdquo; some data on your own. Examples of data tasks that you should be able to do include reshaping data between wide and long formats.\nTextbook There is no required textbook for this course. Highly recommended resources are The Visual Display of Quantitative Information by Edward Tufte, and Communicating Data with Tableau by Ben Jones.\n   Tufte Jones          We will be looking at examples from these texts over the duration of the semester.\nSoftware We will be using the Tableau software (latest version: 10.2 as of Fall 2017) extensively throughout this semester. Tableau is quickly becoming the industry standard for data visualization, as its drag-and-drop user interface is accessible to people who may be uncomfortable with command-line coding. It is also a powerful way to interact with data through the creation of dashboards. Tableau Desktop 10.0 is not usually free, but is available to you as students through the WSU network. We will be uploading vizzes onto Tableau Public, which is free to anybody. Time permitting we will also cover the R package ggplot2. This is the standard for graphics within R, and is better suited for static statistical graphics than Tableau.\nLearning outcomes  Students will be able to identify and describe the methods and techniques commonly used to describe data both numerically and visually. Students will be able to use appropriate methods to summarize and visualize data based upon the data type and the goals of the analysis. Students will be able to demonstrate the ability to clean and prepare data for the summarization and visualization process. Students will demonstrate the ability to summarize and visualize relationships between variables appropriately given the data type of the variables involved. Students will be able to construct an interactive data dashboard to summarize and display the important features of a data set.  Assessments  Attendance and participation (10%)   DSCI 310 will be heavy on in-class discussion and visualization designs. It is important that all students are a part of these discussions. There will also be many in-class data visualization activities in which it is important for you to participate. Accordingly, I will take attendance almost every day throughout the semester and it will be weighted at 10% of your final grade.\n  Visualization critiques (15%)   Students will be required to submit frequent critiques of assigned visualizations. At the beginning of the semester, critiques will mostly be of visualizations I provide you. As the semester progresses, you will also critique your peers' visualizations using the peer grading resouce CrowdGrader. Becoming an effective critic is imperative for your own growth as a visualization designer; hence completing thorough critiques is a non-trivial component of your final grade. I will provide you with a rubric to use when you critique your peers' visualizations.\n  Exam (10%)   There will be one exam in this course, covering visual theory and best practices as well as visualization creation. The exam will assess students‚Äô abilities to critique visualizations, and suggest improvements, by applying perceptual theory and other concepts covered in class.\n  Design tasks (15%)   There will be frequent design tasks where students will be presented with a data set and asked to create a viz summarizing important aspects of the data set. Some design tasks will come with their own questions of interest, while others may require the student to explore the data in order to find and visualize their own questions of interest. Some design tasks may require a nontrivial amount of data wrangling to get the data into a format suitable for visualizing. All design tasks will include an editing process during which your visualization is critiqued by at least 2 peers and you will have a chance to edit your visualization as a result of these critiques.\n  Midterm group project (20%)   There will be one midterm group project at various points in the semester. This projects will consist of oral presentations of the projects to the entire class.\n  Final project (30%)   Each student will be required to find their own data set and visualize it using a series of interactive Dashboards. Each student will prepare a presentation for the class providing the context of their data set, questions posed by the data, and visualizations that communicate the answers to those questions. The data set and a set of preliminary questions must be approved by the instructor.\n Tentative course topics  Principles of effective data visualization   Historical background and examples How we perceive information: Elementary perceptual tasks (EPT) What makes a strong graphic? What makes a poor graphic? Designing for aesthetic Data Type (categorical, ordinal, continuous) Matching data type to the appropriate EPT  Visualizing univariate data   Visualization techniques for single categorical field Visualization techniques for single quantitative field  Visualizing multivariate data   Mosaic and stacked bar charts Time-series plots 2- and 3-way Scatterplots  Visualizing aggregated data   Rates Ratios  Visualizing spatial data   Filled maps Shape-size maps  Data Dashboards   The power of interaction Interaction via filter Interaction via parameterization View filtering  ","date":1503187200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1503187200,"objectID":"34e7554008590f72fa3fa98fc8df0bc9","permalink":"/courses/dsci310/syllabus/","publishdate":"2017-08-20T00:00:00Z","relpermalink":"/courses/dsci310/syllabus/","section":"courses","summary":"Data summarization and visualization (Fall 2017) Course overview The amount and extent of data in our world is growing extremely rapidly. As such there is an increasing need for analysts skilled in summarizing data.","tags":null,"title":"Syllabus (Fall 2017)","type":"courses"},{"authors":["Silas Bergen","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Create your slides in Markdown - click the Slides button to check out the example.   Supplementary notes can be added here, including code, math, and images.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"ff6a19061a984819d30c916886db56ef","permalink":"/publication/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/example/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":[],"title":"An example conference paper","type":"publication"}]