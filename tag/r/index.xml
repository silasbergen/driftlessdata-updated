<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R | Silas Bergen</title>
    <link>https://silasbergen.github.io/testsite/tag/r/</link>
      <atom:link href="https://silasbergen.github.io/testsite/tag/r/index.xml" rel="self" type="application/rss+xml" />
    <description>R</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Â© 2021 Silas Bergen</copyright><lastBuildDate>Tue, 08 Jan 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://silasbergen.github.io/testsite/images/icon_hu88e77fda8259aec1ad610bc9dd60fa00_1241_512x512_fill_lanczos_center_2.png</url>
      <title>R</title>
      <link>https://silasbergen.github.io/testsite/tag/r/</link>
    </image>
    
    <item>
      <title>A word in favor of summarytools</title>
      <link>https://silasbergen.github.io/testsite/post/summarytools/</link>
      <pubDate>Tue, 08 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://silasbergen.github.io/testsite/post/summarytools/</guid>
      <description>&lt;p&gt;Yesterday, I was preparing material for STAT 405 (biostatistics) I am teaching this spring, and was on the prowl for something that is an improvement upon the base R &lt;code&gt;summary()&lt;/code&gt; function (it doesn&amp;rsquo;t even give standard deviations!).  The ideal package would also improve upon the base R &lt;code&gt;table()&lt;/code&gt; method, for which getting row and/or column percents is a huge pain. Base function &lt;code&gt;xtabs()&lt;/code&gt; is great for getting arrays of contigency tables, but no percents.  My first stop was the &lt;a href=&#34;https://cran.r-project.org/web/packages/Hmisc/Hmisc.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;Hmisc&lt;/code&gt; package&lt;/a&gt;, which has a good summary method via its &lt;code&gt;describe()&lt;/code&gt; function.&lt;br&gt;
To demonstrate I use the &lt;a href=&#34;http://www.biostat.ucsf.edu/vgsm/data.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Western Collaborative Group Survey (WCGS)&lt;/a&gt; data from Eric Vittingoff&amp;rsquo;s excellent book &lt;em&gt;Regression Methods in Biostatistics&lt;/em&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;Hmisc::describe(wcgs[,1:5])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## wcgs[, 1:5] 
## 
##  5  Variables      3154  Observations
## --------------------------------------------------------------------------------
## age 
##        n  missing distinct     Info     Mean      Gmd      .05      .10 
##     3154        0       21    0.996    46.28    6.256       39       40 
##      .25      .50      .75      .90      .95 
##       42       45       50       55       57 
## 
## lowest : 39 40 41 42 43, highest: 55 56 57 58 59
## --------------------------------------------------------------------------------
## arcus 
##        n  missing distinct     Info      Sum     Mean      Gmd 
##     3152        2        2    0.628      941   0.2985    0.419 
## 
## --------------------------------------------------------------------------------
## behpat 
##        n  missing distinct 
##     3154        0        4 
##                                   
## Value         A1    A2    B3    B4
## Frequency    264  1325  1216   349
## Proportion 0.084 0.420 0.386 0.111
## --------------------------------------------------------------------------------
## bmi 
##        n  missing distinct     Info     Mean      Gmd      .05      .10 
##     3154        0      679        1    24.52    2.803    20.59    21.52 
##      .25      .50      .75      .90      .95 
##    22.96    24.39    25.84    27.45    28.73 
## 
## lowest : 11.19061 15.66050 16.87200 17.21633 17.22242
## highest: 36.04248 37.22973 37.24805 37.65281 38.94737
## --------------------------------------------------------------------------------
## chd69 
##        n  missing distinct 
##     3154        0        2 
##                       
## Value         No   Yes
## Frequency   2897   257
## Proportion 0.919 0.081
## --------------------------------------------------------------------------------
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It also has a &lt;code&gt;summary&lt;/code&gt; method for objects of class &lt;code&gt;formula&lt;/code&gt; which ultimately can be used to create tables that are ready for markdown:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(chd69~agec, data = wcgs,method=&#39;reverse&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 
## Descriptive Statistics by chd69
## 
## +------------+-----------+-----------+
## |            |No         |Yes        |
## |            |(N=2897)   |(N=257)    |
## +------------+-----------+-----------+
## |agec : 35-40|18%  ( 512)|12%  (  31)|
## +------------+-----------+-----------+
## |    41-45   |36%  (1036)|21%  (  55)|
## +------------+-----------+-----------+
## |    46-50   |23%  ( 680)|27%  (  70)|
## +------------+-----------+-----------+
## |    51-55   |16%  ( 463)|25%  (  65)|
## +------------+-----------+-----------+
## |    56-60   | 7%  ( 206)|14%  (  36)|
## +------------+-----------+-----------+
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It does not work as well as you would expect for additional dimensions, however:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;summary(chd69~agec+behpat, data = wcgs,method=&#39;reverse&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## 
## Descriptive Statistics by chd69
## 
## +------------+-----------+-----------+
## |            |No         |Yes        |
## |            |(N=2897)   |(N=257)    |
## +------------+-----------+-----------+
## |agec : 35-40|18%  ( 512)|12%  (  31)|
## +------------+-----------+-----------+
## |    41-45   |36%  (1036)|21%  (  55)|
## +------------+-----------+-----------+
## |    46-50   |23%  ( 680)|27%  (  70)|
## +------------+-----------+-----------+
## |    51-55   |16%  ( 463)|25%  (  65)|
## +------------+-----------+-----------+
## |    56-60   | 7%  ( 206)|14%  (  36)|
## +------------+-----------+-----------+
## |behpat : A1 | 8%  ( 234)|12%  (  30)|
## +------------+-----------+-----------+
## |    A2      |41%  (1177)|58%  ( 148)|
## +------------+-----------+-----------+
## |    B3      |40%  (1155)|24%  (  61)|
## +------------+-----------+-----------+
## |    B4      |11%  ( 331)| 7%  (  18)|
## +------------+-----------+-----------+
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Enter &lt;a href=&#34;https://cran.r-project.org/web/packages/summarytools/vignettes/Introduction.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;summarytools&lt;/a&gt;, immediately appealing in its simplicity.  Indeed it only has four primary functions, centered on its wonderful &lt;code&gt;dfSummary()&lt;/code&gt; function:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(summarytools)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Registered S3 method overwritten by &#39;pryr&#39;:
##   method      from
##   print.bytes Rcpp
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## For best results, restart R session and update pander using devtools:: or remotes::install_github(&#39;rapporter/pander&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../img/dfsummary1.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;This is the prettiest, most thorough output I&amp;rsquo;ve come across in a summary function, complete with ASCII bar graphs or histograms representing categorical or quantitative variables.  You can prettify it even further in the browser with the &lt;code&gt;view()&lt;/code&gt; command:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;view(dfSummary(wcgs))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;../../img/dfsummary2.PNG&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Two-way tables come by way of &lt;code&gt;ctable()&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ctable(wcgs$agec,wcgs$chd69)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Cross-Tabulation, Row Proportions  
## agec * chd69  
## Data Frame: wcgs  
## 
## ------- ------- -------------- ------------- ---------------
##           chd69             No           Yes           Total
##    agec                                                     
##   35-40            512 (94.3%)    31 ( 5.7%)    543 (100.0%)
##   41-45           1036 (95.0%)    55 ( 5.0%)   1091 (100.0%)
##   46-50            680 (90.7%)    70 ( 9.3%)    750 (100.0%)
##   51-55            463 (87.7%)    65 (12.3%)    528 (100.0%)
##   56-60            206 (85.1%)    36 (14.9%)    242 (100.0%)
##   Total           2897 (91.9%)   257 ( 8.1%)   3154 (100.0%)
## ------- ------- -------------- ------------- ---------------
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A couple drawbacks to &lt;code&gt;summarytools&lt;/code&gt;: (1) It is not very compatible with the tidyverse, as you can see with the above use of &lt;code&gt;ctable()&lt;/code&gt;.  (2) Even the nice 2x2 table is not easily extentable to higher dimensions.  You could use &lt;code&gt;by()&lt;/code&gt;, but&amp;hellip;who wants to do that?&lt;/p&gt;
&lt;p&gt;In finishing this post I see Adam Medcalf from Dabbling with Data has a &lt;a href=&#34;https://dabblingwithdata.wordpress.com/2018/01/02/my-favourite-r-package-for-summarising-data/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;nice post&lt;/a&gt; on Hmisc, summarytools, and a couple others as well.  For my money, I&amp;rsquo;ll take summarytools, though I wish its beautiful 2x2 table displays were more easily extended and its &lt;code&gt;ctable()&lt;/code&gt; and &lt;code&gt;descr()&lt;/code&gt; functions more tidyverseable!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Stat consulting: a data science playground</title>
      <link>https://silasbergen.github.io/testsite/post/dsci-consulting/</link>
      <pubDate>Wed, 02 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://silasbergen.github.io/testsite/post/dsci-consulting/</guid>
      <description>&lt;p&gt;This past semester I taught our &lt;a href=&#34;../../courses/stat370/stat370-home/&#34;&gt;STAT 370 (statistical consulting and communication)&lt;/a&gt; for the first time.  This course gave student experience consulting for real clients from the university and community and focused on communicating with a client as well as report and presentation preparation best practices.  Most of the required analyses were simple: paired t-tests, simple linear regression, etc.  What struck me was the nontrivality of the data tidying process!  While STAT 370 is taken mostly by our statistics majors, so many of the examples we encountered would be beautiful case studies for our introductory DSCI (data science) curriculum. In this post I present an actual example from a client that illustrates this.&lt;/p&gt;
&lt;p&gt;The data here concerned undergraduate nursing students in one of four terms of the nursing program.  Of interest was measuring the students&#39; resiliency as measured by the Connor-Davidson Resiliency Scale (CDRS), both prior to and following impelementation of a Stress Management and Resiliency Training (SMART).  The client was interested in determining for which of the four terms was there a significant change in resilience.&lt;/p&gt;
&lt;p&gt;Here are the data we&amp;rsquo;re working with (&lt;a href=&#34;../../files/cdrs_data_pre.csv&#34;&gt;link to pre&lt;/a&gt; | &lt;a href=&#34;../../files/cdrs_data_post.csv&#34;&gt;link to post&lt;/a&gt;). &lt;code&gt;id&lt;/code&gt; stands for a unique student identifier.  We also have responses to 18 of the CDRS items (each on a 5-point Likert scale).  The &lt;code&gt;post&lt;/code&gt; data set also contains the student terms; notably this information is &lt;em&gt;not&lt;/em&gt; available in the &lt;code&gt;pre&lt;/code&gt; data set.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;library(dplyr)
library(tidyr)
library(ggplot2)
pre &amp;lt;- read.csv(&#39;cdrs_data_pre.csv&#39;)
post &amp;lt;- read.csv(&#39;cdrs_data_post.csv&#39;)
head(post)
names(pre) #missing term information!!
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    id  Term CDRS_Q1 CDRS_Q2 CDRS_Q3 CDRS_Q4 CDRS_Q5 CDRS_Q6 CDRS_Q7 CDRS_Q8
## 1  23 Term1       4       4       3       4       4       3       3       4
## 2 183 Term1       3       4       4       3       4       3       4       2
## 3  80 Term1       3       4       4       3       4       3       4       3
## 4   1 Term1       3       4       4       3       3       2       4       4
## 5 166 Term1       3       4       3       4       4       4       4       4
## 6  15 Term1       3       3       2       3       3       2       3       3
##   CDRS_Q9 CDRS_Q10 CDRS_Q11 CDRS_Q12 CDRS_Q20 CDRS_Q21 CDRS_Q22 CDRS_Q23
## 1       4        4        4        3        2        3        3        3
## 2       3        3        3        3        3        3        3        3
## 3       4        4        4        4        4        4        4        3
## 4       4        4        4        4        2        3        3        2
## 5       4        4        4        4        3        4        4        4
## 6       2        3        3        3        2        3        3        3
##   CDRS_Q24 CDRS_Q25
## 1        4        4
## 2        4        4
## 3        4        4
## 4        3        4
## 5        4        4
## 6        3        3
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;id&amp;quot;       &amp;quot;CDRS_Q1&amp;quot;  &amp;quot;CDRS_Q2&amp;quot;  &amp;quot;CDRS_Q3&amp;quot;  &amp;quot;CDRS_Q4&amp;quot;  &amp;quot;CDRS_Q5&amp;quot; 
##  [7] &amp;quot;CDRS_Q6&amp;quot;  &amp;quot;CDRS_Q7&amp;quot;  &amp;quot;CDRS_Q8&amp;quot;  &amp;quot;CDRS_Q9&amp;quot;  &amp;quot;CDRS_Q10&amp;quot; &amp;quot;CDRS_Q11&amp;quot;
## [13] &amp;quot;CDRS_Q12&amp;quot; &amp;quot;CDRS_Q20&amp;quot; &amp;quot;CDRS_Q21&amp;quot; &amp;quot;CDRS_Q22&amp;quot; &amp;quot;CDRS_Q23&amp;quot; &amp;quot;CDRS_Q24&amp;quot;
## [19] &amp;quot;CDRS_Q25&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the data are not &lt;a href=&#34;https://www.jstatsoft.org/article/view/v059i10&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tidy&lt;/a&gt; in the sense that each row is a person, and we have variable information on the questions in columns.  I&amp;rsquo;ll return to this in a bit.&lt;/p&gt;
&lt;p&gt;Here ultimately is a visualization that we could use to determine for which terms are the SMART effects strongest, and for which terms is the effect statistically significant:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://silasbergen.github.io/testsite/post/dsci-consulting_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can see that the average resilience pre-SMART was lower than average resilience post-SMART, and that these differences were most extreme for students in Terms 2 and 3 (which were also the only statistically significant differences).  Additionally, students in Term 4 had very high pre- and post-SMART resilience (they&amp;rsquo;re seasoned veterans, after all!)&lt;/p&gt;
&lt;p&gt;A simple plot, with a simple interpretation.  But the path to get there is anything but!  To create this plot we need:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;to average the 18 CDRS items for each student;&lt;/li&gt;
&lt;li&gt;join the data sets;&lt;/li&gt;
&lt;li&gt;compute paired t-tests for each term;&lt;/li&gt;
&lt;li&gt;prepare data for plotting;&lt;/li&gt;
&lt;li&gt;plot&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So, let&amp;rsquo;s proceed!&lt;/p&gt;
&lt;h2 id=&#34;average-the-cdrs-items&#34;&gt;Average the CDRS items&lt;/h2&gt;
&lt;p&gt;This is perhaps the most interesting step in the process.  As mentioned earlier, the data are not tidy in the sense that we have variable information in columns instead of rows.  We could reshape (&amp;ldquo;gather&amp;rdquo; or &amp;ldquo;melt&amp;rdquo;) to average CDRS score by term.  Doing this for the post data set:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;post_melt &amp;lt;- post %&amp;gt;% 
  gather(key = &#39;Question&#39;,value=&#39;Response&#39;,CDRS_Q1:CDRS_Q25)
head(post_melt)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    id  Term Question Response
## 1  23 Term1  CDRS_Q1        4
## 2 183 Term1  CDRS_Q1        3
## 3  80 Term1  CDRS_Q1        3
## 4   1 Term1  CDRS_Q1        3
## 5 166 Term1  CDRS_Q1        3
## 6  15 Term1  CDRS_Q1        3
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;post_melt %&amp;gt;% 
  group_by(id,Term) %&amp;gt;%
  summarize(post_mean = mean(Response))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` regrouping output by &#39;id&#39; (override with `.groups` argument)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 67 x 3
## # Groups:   id [67]
##       id Term  post_mean
##    &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;     &amp;lt;dbl&amp;gt;
##  1     1 Term1      3.33
##  2     2 Term3      3   
##  3     4 Term2      2.83
##  4     8 Term2      3.11
##  5     9 Term1      2.72
##  6    13 Term4      2.72
##  7    15 Term1      2.78
##  8    20 Term1      3   
##  9    23 Term1      3.5 
## 10    26 Term4      2.56
## # ... with 57 more rows
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We could also forego the melting, since ultimately all we want is the average response for each student.  We can use the original &lt;code&gt;post&lt;/code&gt; data set, and an application of the &lt;code&gt;rowMeans()&lt;/code&gt; function within which is nested the &lt;code&gt;select()&lt;/code&gt; command:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;post2 &amp;lt;- post %&amp;gt;%
  mutate(post_mean = rowMeans(select(.,CDRS_Q1:CDRS_Q25))) %&amp;gt;%
  select(id, Term, post_mean)
head(post2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    id  Term post_mean
## 1  23 Term1  3.500000
## 2 183 Term1  3.277778
## 3  80 Term1  3.722222
## 4   1 Term1  3.333333
## 5 166 Term1  3.833333
## 6  15 Term1  2.777778
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;One could argue that the first approach (using &lt;code&gt;gather()&lt;/code&gt;) is the best, since it first creates a &amp;ldquo;tidy&amp;rdquo; data set and is arguably more readable.  But the second appears more succinct.  I&amp;rsquo;ll use the second approach to carry out the same task on the &lt;code&gt;pre&lt;/code&gt; data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;pre2 &amp;lt;- pre %&amp;gt;%
  mutate(pre_mean = rowMeans(select(.,CDRS_Q1:CDRS_Q25))) %&amp;gt;%
  select(id, pre_mean)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;join-pre-and-post&#34;&gt;Join pre and post&lt;/h2&gt;
&lt;p&gt;This step is easy!&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;both &amp;lt;- inner_join(pre2,post2,by=&#39;id&#39;)
head(both)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   id pre_mean  Term post_mean
## 1  1 3.333333 Term1  3.333333
## 2  2 2.944444 Term3  3.000000
## 3  4 2.888889 Term2  2.833333
## 4  8 2.944444 Term2  3.111111
## 5  9 2.444444 Term1  2.722222
## 6 13 2.944444 Term4  2.722222
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;carry-out-paired-t-tests-by-term&#34;&gt;Carry out paired t-tests by term&lt;/h2&gt;
&lt;p&gt;Here is the lone &amp;ldquo;formal&amp;rdquo; statistical aspect of the whole problem!  We can carry out paired t-tests by term as follows using some ugly base-R code: the &lt;code&gt;by()&lt;/code&gt; command.  I won&amp;rsquo;t show the output, but here we can see that only for Terms 2 and 3 is the SMART effect statistically significant:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;by(both, both$Term, function(df) t.test(df$pre_mean,df$post_mean,paired=TRUE))
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;prepare-data-for-plotting&#34;&gt;Prepare data for plotting&lt;/h2&gt;
&lt;p&gt;Referring back to the figure, the geometric mapping includes four points for the four pre-SMART CDRS averages (one for each term); four points for the post-SMART CDRS averages (one for each term); and a line segment connecting them.  We can use the joined data set to form our &amp;ldquo;plotting&amp;rdquo; data set.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;toplot &amp;lt;- both %&amp;gt;%
  group_by(Term) %&amp;gt;%
  summarize(avg_pre = mean(pre_mean),avg_post = mean(post_mean)) %&amp;gt;% 
  mutate(sig = c(&#39;no&#39;,&#39;yes&#39;,&#39;yes&#39;,&#39;no&#39;))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## `summarise()` ungrouping output (override with `.groups` argument)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;toplot
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4 x 4
##   Term  avg_pre avg_post sig  
##   &amp;lt;chr&amp;gt;   &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;
## 1 Term1    2.97     3.10 no   
## 2 Term2    2.98     3.16 yes  
## 3 Term3    2.95     3.26 yes  
## 4 Term4    3.21     3.2  no
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Note that the &lt;code&gt;avg_pre&lt;/code&gt; and &lt;code&gt;avg_post&lt;/code&gt; columns are in fact &amp;ldquo;means of means,&amp;rdquo; and we have manually added a column to indicate whether the difference was statistically significant.&lt;/p&gt;
&lt;h2 id=&#34;plot&#34;&gt;Plot!&lt;/h2&gt;
&lt;p&gt;And now the fun begins!  Here is ggplot code to create the original figure:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ggplot(data = toplot) + 
  geom_point(aes(x = avg_pre, y = 1:4, shape=&#39;a&#39;,color=sig),size=2)  + 
  geom_point(aes(x = avg_post, y = 1:4,shape=&#39;b&#39;,color=sig),size=2) + 
  geom_segment(aes(x = avg_pre, xend = avg_post, y=1:4,yend=1:4,color=sig)) + 
  scale_y_reverse() + xlab(&#39;average CDRS&#39;) + ylab(&#39;Term&#39;) + 
  scale_shape_manual(name=&#39;&#39;,values=c(&#39;a&#39;=19,&#39;b&#39;=17),labels=c(&#39;pre SMART&#39;,&#39;post SMART&#39;)) + 
  scale_color_discrete(name=&#39;significant?&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://silasbergen.github.io/testsite/post/dsci-consulting_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;576&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;moral&#34;&gt;Moral&lt;/h2&gt;
&lt;p&gt;Wow!  The only truly &amp;ldquo;statistical&amp;rdquo; aspect of this whole process was a paired t-test.  And even that was a bit tricky: figuring out how to carry out these t-tests by term!  In reflecting back on the semester, I am struck that our data science students would do well to take our stat consulting course.  They would be kept very interested in data &amp;ldquo;wrangling&amp;rdquo; tasks such as this. On the flip side, it&amp;rsquo;s an invaluable experience for our STAT majors (who comprise the usual audience of this course) to realize that the formal statistical analysis in which they are most trained is ultimately the last in a long series of data wrangling steps.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Quantifying thrill</title>
      <link>https://silasbergen.github.io/testsite/post/viz-worldseries-wpa/</link>
      <pubDate>Tue, 31 Oct 2017 00:00:00 -0500</pubDate>
      <guid>https://silasbergen.github.io/testsite/post/viz-worldseries-wpa/</guid>
      <description>&lt;p&gt;Monday morning, October 30, found me groggy and sandy-eyed.  The culprit was the 5-hour and 17-minute, 10-inning thriller between the LA Dodgers and Houston Astros in Game 5 of the 2017 the night before.  Thanks to living in the Central Time Zone, I went to bed around 1am. The Astros ended up defeating the Dodgers 13-12, but the game was insane, featuring three comebacks from deficits of 3 runs or more.  By the end, I was emotionally exhausted, and I didn&amp;rsquo;t even have a stake in either team!  Lots was written about the game over at &lt;a href=&#34;http://www.fangraphs.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fangraphs&lt;/a&gt;, one of my favorite baseball analytics websites.&lt;/p&gt;
&lt;p&gt;One &lt;a href=&#34;https://www.fangraphs.com/blogs/game-five-was-as-weird-as-it-felt/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;post in particular&lt;/a&gt; by Craig Edwards caught my attention from a data visualization perspective.  It compared the game to another epic game: &lt;a href=&#34;https://www.baseball-reference.com/boxes/SLN/SLN201110270.shtml&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Game 6 of the 2011 World Series&lt;/a&gt;, where the St. Louis Cardinals staved off elimination by defeating the Texas Rangers 10-9 in 11 innings.  The crux of the article was a table that listed the top-20 &amp;ldquo;most exciting events&amp;rdquo; of each World Series side-by-side.  Here is a screenshot of said table:&lt;/p&gt;
&lt;center&gt;
&lt;figure&gt; 
&lt;img src=&#34;https://silasbergen.github.io/testsite/img/wpa-table.JPG&#34; width=&#34;400&#34; /&gt;
&lt;/figure&gt;
&lt;/center&gt;
&lt;p&gt;An &amp;ldquo;exciting event&amp;rdquo; is one that yields a large swing in the win expectancy of the game, as measured by Win Probability Added, or WPA.  Read Fangraph&amp;rsquo;s &lt;a href=&#34;https://www.fangraphs.com/library/misc/wpa/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;excellent glossary entry on WPA&lt;/a&gt; for more detail, but essentially, the bigger the WPA of an event, the more exciting it is.&lt;/p&gt;
&lt;p&gt;The table is great, but it&amp;rsquo;s hard to see which of the two games has the &lt;em&gt;most&lt;/em&gt; exciting Top-20 events.  I wanted to visualize!  But I needed the data.  Fangraphs has play logs for every single Major League game, which lists (among other metrics) the events of the game; the win expectancy following the event; and the WPA of the event.  I needed data from the &lt;a href=&#34;https://www.fangraphs.com/plays.aspx?date=2011-10-27&amp;amp;team=Cardinals&amp;amp;dh=0&amp;amp;season=2011&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Game 6, 2011&lt;/a&gt; and the &lt;a href=&#34;https://www.fangraphs.com/plays.aspx?date=2017-10-29&amp;amp;team=Astros&amp;amp;dh=0&amp;amp;season=2017&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Game 5, 2017&lt;/a&gt; play logs.  But I needed them both in the same data source!&lt;/p&gt;
&lt;p&gt;&lt;code&gt;R&lt;/code&gt; has a great package &lt;code&gt;rvest&lt;/code&gt; that makes web-scraping (especially scraping html tables) quite easy.  Here&amp;rsquo;s the code I wrote to scrape the &lt;a href=&#34;https://www.fangraphs.com/plays.aspx?date=2011-10-27&amp;amp;team=Cardinals&amp;amp;dh=0&amp;amp;season=2011&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Game 6, 2011&lt;/a&gt; data; do some cleaning; and write the cleaned data into a .csv file.  I wrote very similar code to get the &lt;a href=&#34;https://www.fangraphs.com/plays.aspx?date=2017-10-29&amp;amp;team=Astros&amp;amp;dh=0&amp;amp;season=2017&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Game 5, 2017&lt;/a&gt; data:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;library(rvest)
library(dplyr)
#Read in the data, find the right table:
url &amp;lt;- &#39;https://www.fangraphs.com/plays.aspx?date=2011-10-27&amp;amp;team=Cardinals&amp;amp;dh=0&amp;amp;season=2011&#39;
raw &amp;lt;- read_html(url)%&amp;gt;%
       html_table(fill=TRUE)
mytable &amp;lt;- raw[[9]][,1:12]

#Use dplyr to clean it up. By code row:
  #Create absolute value of WPA
  #Create new column to indicate the game
  #Remove the &amp;quot;%&amp;quot; from the win expectancy, create Event number
  #Arrange in descending order by WPA
  #Create rank column
cleantable &amp;lt;- mytable %&amp;gt;%
  mutate(WPA_abs = abs(WPA)) %&amp;gt;%                                            
  mutate(Game = rep(&#39;Game 6, 2011&#39;,nrow(mytable))) %&amp;gt;%                    
  mutate(WE = as.numeric(gsub(&#39;%&#39;,&#39;&#39;,WE)), Event = 1:nrow(mytable)) %&amp;gt;%      
  arrange(-WPA_abs) %&amp;gt;%                                                     
  mutate(WPA_Rank = 1:nrow(mytable))                                          

#Write cleaned data to csv file
write.csv(cleantable,file=&#39;WPA_Game6_2011_all.csv&#39;,row.names=FALSE)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then on to visualizing!  Using Tableau, I created a win-expectancy graph for each of the games.  The red dots indicate &amp;ldquo;exciting events&amp;rdquo;; events with WPA of 15% or more:&lt;/p&gt;
&lt;center&gt;
&lt;iframe src=&#34;https://public.tableau.com/views/WPA_WS_blog/tracker-dash?:embed=y?:showVizHome=no&#34;
 width=&#34;755&#34; height=&#34;392&#34;&gt;&lt;/iframe&gt;
&lt;/center&gt;
&lt;p&gt;Clearly, both games were crazy, with wild swings in win expectancy!  If you count up the red dots, Game 6 in 2011 had 8 &amp;ldquo;exciting events&amp;rdquo; while Game 5 in 2017 had 10 &amp;ldquo;exciting events.&amp;rdquo;  15% is quite an arbitrary threshold for &amp;ldquo;exciting&amp;rdquo; however; different thresholds would likely change the comparison.  Comparing total WPA flips the story: the total WPA of 7.2 in Game 6, 2011 was larger than the total WPA of 6.2 in Game 5, 2017.&lt;/p&gt;
&lt;p&gt;My primary interest, however, was to compare the top-20 most exciting events of both games in a clearer visual manner, respecting principles of human perception:&lt;/p&gt;
&lt;iframe src=&#34;https://public.tableau.com/views/WPA_WS_blog/Top-20?:embed=y?:showVizHome=no&#34;
 width=&#34;655&#34; height=&#34;495&#34;&gt;&lt;/iframe&gt;
&lt;p&gt;From this graph, we can see the four points on the right that lie below the reference line.  These indicate that the top four most exciting events in Game 6, 2011 were &lt;em&gt;more exciting&lt;/em&gt; than the top four most exciting events from Game 5, 2017.  On the other hand, the top 5-10 most exciting events in Game 5, 2017 lie above the reference line, indicating they were more exciting than the events with WPA ranked 5-10 in Game 6, 2011.  The rest of the events ranked 11-20 in WPA go back to lying below the line.&lt;/p&gt;
&lt;p&gt;So, it does appear that Game 6, 2011 was truly a more thrilling game than Game 5, 2017!   The total WPA of Game 6, 2011 was greater than the total WPA in Game 5, 2017, and of the top-20 &amp;ldquo;most exciting&amp;rdquo; events, they tended to be &lt;em&gt;more exciting&lt;/em&gt; in 2011. Oh well, Game 5, 2017 was still worth losing a few hours of sleep!  I think&amp;hellip;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
